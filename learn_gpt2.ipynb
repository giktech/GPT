{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learn_gpt2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHGfRPoF05X1LXkPZlXMug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giktech/GPT/blob/master/learn_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B7WcZN9XSGK",
        "colab_type": "text"
      },
      "source": [
        "colab notebook(python 3) stored in git.\n",
        "Trains gpt2 on a couple of kids books and uses it to generate pieces of a story\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNNBA9fcw4RD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1a044b33-318e-49d2-9e78-15c38c3765b6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCGBa9gvKeKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cf9570c-2af9-40f4-d377-fcd148c3bcee"
      },
      "source": [
        "%cd drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNpcKKINLTKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5376612c-646c-4ff0-cd03-aeaaacf60e65"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDGQugmGLbr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "484cc1bd-9004-4e16-9f0d-e16604b365d0"
      },
      "source": [
        "%cd My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB4CNdGELw7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0d8a804a-a6d1-4800-a7e1-7db88bbe341c"
      },
      "source": [
        "#%mkdir try_gpt\n",
        "%cd /content/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxaulvujMXej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "e11606e5-415b-4a7c-90c0-5ad9328d6744"
      },
      "source": [
        "!git clone https://github.com/mohamad-ali-nasser/gpt-2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Total 395 (delta 0), reused 0 (delta 0), pack-reused 395\u001b[K\n",
            "Receiving objects: 100% (395/395), 4.43 MiB | 2.54 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlNs4ssqMhjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1976db63-2a9c-4d83-f796-a1e97e4c25c0"
      },
      "source": [
        "%cd gpt-2\n",
        "!python3 download_model.py 345M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n",
            "Fetching checkpoint: 1.00kit [00:00, 1.00Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 66.5Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.19Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:48, 29.4Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 8.95Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 55.8Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 55.6Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9iVeLbGMle_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3d9f0b5f-2861-4ed6-ee59-34e4115b551f"
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONTRIBUTORS.md  Dockerfile.gpu     LICENSE    requirements.txt  train.py\n",
            "DEVELOPERS.md\t download_model.py  models     src\n",
            "Dockerfile.cpu\t encode.py\t    README.md  train-horovod.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5atPlweN9AR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c62efdb-7f6a-4978-a1ca-e4a3dcc7d080"
      },
      "source": [
        "!ls ./models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "345M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOTm8raBOGNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2075de55-4640-47a1-8503-7782915b444b"
      },
      "source": [
        "!ls ./models/345M/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint    hparams.json\t\t      model.ckpt.index\tvocab.bpe\n",
            "encoder.json  model.ckpt.data-00000-of-00001  model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8d6iKUOJPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5eac34ca-ddd4-49ef-cd4b-8c4e598b99c9"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XReFjOfROzBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "15815cb3-4a3e-4ffd-bb54-f32ad2ece283"
      },
      "source": [
        "%cd src\n",
        "%mkdir corpus\n",
        "%cd corpus/\n",
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2/src\n",
            "/content/gpt-2/src/corpus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ7nAD27O8Wm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dce9ce19-7728-428e-ef89-64c0a28a3a21"
      },
      "source": [
        "%cd /content/gpt-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0LuwjhUnaa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "d15087c6-8816-45db-e63b-c1cf5c3379e1"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.8MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 18.4MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=bd7ca9a17b6ac287aff2c522b6404d318992d0d22c221ed7b37468d763a3593b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533204 sha256=97565f3d206b56e4b4990e861b90b9347c41159109fb127d5dff6a438a4060f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, idna, requests, tqdm, toposort\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed fire-0.3.1 idna-2.8 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "requests",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMjMXQD8nzaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e8cd638-31f8-4f25-ec72-4dab0749aefe"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset src/corpus/corpus.txt --model_name '345M'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: ./train.py: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivcZtEAOohy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31efaebb-6514-4d02-bf3d-caf535a26e5d"
      },
      "source": [
        "!pwd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7derDdolcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80acfdc0-47ba-4c57-9619-e908a00215c3"
      },
      "source": [
        "cd /content/gpt-2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8i0x4cZpGH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ff805cbc-b982-434b-ea52-21fc370c49ff"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONTRIBUTORS.md  Dockerfile.gpu     LICENSE    requirements.txt  train.py\n",
            "DEVELOPERS.md\t download_model.py  models     src\n",
            "Dockerfile.cpu\t encode.py\t    README.md  train-horovod.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXv4vxyepH25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "a75ebb79-f943-4165-9619-0cab6e41398f"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset src/corpus/corpus.txt --model_name '345M'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-08 01:17:56.076303: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"./train.py\", line 14, in <module>\n",
            "    import model, sample, encoder\n",
            "  File \"/content/gpt-2/src/model.py\", line 3, in <module>\n",
            "    from tensorflow.contrib.training import HParams\n",
            "ModuleNotFoundError: No module named 'tensorflow.contrib'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzUGC_EzpUkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29db3db2-f39e-4275-97d9-f5d633e7bd6b"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install 'tensorflow-estimator<1.15.0rc0,>=1.14.0rc0' --force-reinstall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 56.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 27.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.30.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.18.5)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.0) (49.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=5004b6708ca1b0607180db91e5d7aa25e3dce499c067f8ec2ebaf5cc5dfb83ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, gast, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 13.0MB/s \n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-estimator\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHgtRiAop5A4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f52bbdfc-2c78-44a4-b0ac-361775d6eb74"
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-*/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9-0!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-08 01:20:51--  https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.195.57.194\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.195.57.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?h4BLP0aqEywkRuO_KWbEsBDthYMiWbIHehaq-feQ1n9e3A6VBk2IoIlq78Es--dYxPHqUzzCfisEy0PU_rc2stg9b1_T2qneiH_Ko9uw27FA7fv7X-17uedNQNlZGIpuEFO1nZOJgcf-tKijq5czw7Gi2zJ1MlPGYdj7zPmbIlOsNkiw-lfEUdJDeVYSDIyftkgmEgqB-QN5ppSgQhgd [following]\n",
            "--2020-08-08 01:20:52--  https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?h4BLP0aqEywkRuO_KWbEsBDthYMiWbIHehaq-feQ1n9e3A6VBk2IoIlq78Es--dYxPHqUzzCfisEy0PU_rc2stg9b1_T2qneiH_Ko9uw27FA7fv7X-17uedNQNlZGIpuEFO1nZOJgcf-tKijq5czw7Gi2zJ1MlPGYdj7zPmbIlOsNkiw-lfEUdJDeVYSDIyftkgmEgqB-QN5ppSgQhgd\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1212738714 (1.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.13G   163MB/s    in 7.4s    \n",
            "\n",
            "2020-08-08 01:20:59 (156 MB/s) - ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’ saved [1212738714/1212738714]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-0-local.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  Packages [15.4 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Ign:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [882 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,855 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [116 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.1 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,413 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,037 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,334 kB]\n",
            "Get:26 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [895 kB]\n",
            "Fetched 8,087 kB in 5s (1,568 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package cuda-9-0!export\n",
            "E: Couldn't find any package by glob 'cuda-9-0!export'\n",
            "E: Unable to locate package LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/local/cuda-9.0/lib64\n",
            "E: Couldn't find any package by glob 'LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/local/cuda-9.0/lib64'\n",
            "E: Couldn't find any package by regex 'LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/local/cuda-9.0/lib64'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mTQTci2qrPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20077527-5060-458b-e52f-86e04200afb4"
      },
      "source": [
        "!apt-get install cuda-9-0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "0 upgraded, 33 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 0 B/1,097 MB of archives.\n",
            "After this operation, 2,315 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-9-0-local  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  cuda-core-9-0 9.0.176-1 [16.9 MB]\n",
            "Get:4 file:/var/cuda-repo-9-0-local  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:7 file:/var/cuda-repo-9-0-local  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:8 file:/var/cuda-repo-9-0-local  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:9 file:/var/cuda-repo-9-0-local  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:10 file:/var/cuda-repo-9-0-local  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:11 file:/var/cuda-repo-9-0-local  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:12 file:/var/cuda-repo-9-0-local  cuda-cublas-9-0 9.0.176-1 [25.0 MB]\n",
            "Get:13 file:/var/cuda-repo-9-0-local  cuda-cublas-dev-9-0 9.0.176-1 [49.4 MB]\n",
            "Get:14 file:/var/cuda-repo-9-0-local  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:15 file:/var/cuda-repo-9-0-local  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:16 file:/var/cuda-repo-9-0-local  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:17 file:/var/cuda-repo-9-0-local  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:18 file:/var/cuda-repo-9-0-local  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:19 file:/var/cuda-repo-9-0-local  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:20 file:/var/cuda-repo-9-0-local  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:21 file:/var/cuda-repo-9-0-local  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:22 file:/var/cuda-repo-9-0-local  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:23 file:/var/cuda-repo-9-0-local  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:24 file:/var/cuda-repo-9-0-local  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:25 file:/var/cuda-repo-9-0-local  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:26 file:/var/cuda-repo-9-0-local  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:27 file:/var/cuda-repo-9-0-local  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:28 file:/var/cuda-repo-9-0-local  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:29 file:/var/cuda-repo-9-0-local  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:30 file:/var/cuda-repo-9-0-local  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:31 file:/var/cuda-repo-9-0-local  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:32 file:/var/cuda-repo-9-0-local  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:33 file:/var/cuda-repo-9-0-local  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "(Reading database ... 144546 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../02-cuda-core-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../03-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../11-cuda-cublas-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../13-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../15-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../19-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../21-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../22-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../23-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../24-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../25-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../26-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../27-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../28-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../29-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../30-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../31-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../32-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06xVnje7rPR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM9X4ffCrbk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa6e405b-0929-4ed1-cd12-7e045d50d4c3"
      },
      "source": [
        "!pwd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiITpdDDrdC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "229d0cde-5652-4116-bfe8-c16674f12d48"
      },
      "source": [
        "%cd gpt-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1oIKkv3rf5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d4e7d4c-0d59-4480-c882-5c466d2d651c"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset src/corpus/corpus.txt --model_name '345M'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-08-08 01:28:04.800800: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-08-08 01:28:04.877611: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-08-08 01:28:04.880971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2559640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-08 01:28:04.881003: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-08 01:28:04.889926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-08 01:28:05.102573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-08 01:28:05.103312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2558d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-08 01:28:05.103358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-08 01:28:05.104406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-08 01:28:05.104957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-08-08 01:28:05.107230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-08-08 01:28:05.334067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-08-08 01:28:05.417217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-08-08 01:28:05.440676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-08-08 01:28:05.673806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-08-08 01:28:05.802733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-08-08 01:28:06.306846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-08 01:28:06.307043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-08 01:28:06.307734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-08 01:28:06.308287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-08-08 01:28:06.312868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-08-08 01:28:06.314744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-08 01:28:06.314787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-08-08 01:28:06.314805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-08-08 01:28:06.316069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-08 01:28:06.316670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-08 01:28:06.317226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:117: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:156: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "2020-08-08 01:28:29.138774: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "2020-08-08 01:28:29.147409: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "2020-08-08 01:28:29.156266: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "2020-08-08 01:28:29.159020: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "2020-08-08 01:28:29.161823: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [00:26<00:00, 26.45s/it]\n",
            "dataset has 6312984 tokens\n",
            "Training...\n",
            "2020-08-08 01:29:59.865219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 10.66] loss=2.69 avg=2.69\n",
            "[2 | 12.17] loss=3.54 avg=3.12\n",
            "[3 | 13.70] loss=2.75 avg=2.99\n",
            "[4 | 15.22] loss=3.36 avg=3.08\n",
            "[5 | 16.74] loss=2.50 avg=2.97\n",
            "[6 | 18.28] loss=2.79 avg=2.94\n",
            "[7 | 19.81] loss=3.28 avg=2.99\n",
            "[8 | 21.35] loss=3.46 avg=3.05\n",
            "[9 | 22.89] loss=3.56 avg=3.11\n",
            "[10 | 24.42] loss=2.83 avg=3.08\n",
            "[11 | 25.97] loss=2.84 avg=3.06\n",
            "[12 | 27.51] loss=3.40 avg=3.09\n",
            "[13 | 29.06] loss=3.62 avg=3.13\n",
            "[14 | 30.62] loss=3.48 avg=3.16\n",
            "[15 | 32.18] loss=2.74 avg=3.13\n",
            "[16 | 33.74] loss=3.35 avg=3.14\n",
            "[17 | 35.31] loss=3.42 avg=3.16\n",
            "[18 | 36.87] loss=3.42 avg=3.17\n",
            "[19 | 38.44] loss=3.54 avg=3.19\n",
            "[20 | 40.00] loss=3.26 avg=3.20\n",
            "[21 | 41.56] loss=3.60 avg=3.22\n",
            "[22 | 43.14] loss=3.38 avg=3.23\n",
            "[23 | 44.71] loss=3.10 avg=3.22\n",
            "[24 | 46.29] loss=2.93 avg=3.21\n",
            "[25 | 47.87] loss=3.05 avg=3.20\n",
            "[26 | 49.45] loss=3.26 avg=3.20\n",
            "[27 | 51.04] loss=3.55 avg=3.22\n",
            "[28 | 52.62] loss=3.55 avg=3.23\n",
            "[29 | 54.21] loss=3.10 avg=3.23\n",
            "[30 | 55.81] loss=2.91 avg=3.21\n",
            "[31 | 57.40] loss=2.50 avg=3.19\n",
            "[32 | 59.00] loss=2.95 avg=3.18\n",
            "[33 | 60.60] loss=2.75 avg=3.16\n",
            "[34 | 62.21] loss=2.47 avg=3.14\n",
            "[35 | 63.82] loss=3.46 avg=3.15\n",
            "[36 | 65.44] loss=3.32 avg=3.16\n",
            "[37 | 67.05] loss=2.69 avg=3.14\n",
            "[38 | 68.66] loss=2.81 avg=3.13\n",
            "[39 | 70.28] loss=2.66 avg=3.12\n",
            "[40 | 71.91] loss=2.82 avg=3.11\n",
            "[41 | 73.53] loss=3.20 avg=3.11\n",
            "[42 | 75.16] loss=2.79 avg=3.10\n",
            "[43 | 76.79] loss=3.28 avg=3.11\n",
            "[44 | 78.44] loss=3.42 avg=3.11\n",
            "[45 | 80.08] loss=3.05 avg=3.11\n",
            "[46 | 81.72] loss=2.88 avg=3.11\n",
            "[47 | 83.36] loss=2.57 avg=3.09\n",
            "[48 | 85.01] loss=2.99 avg=3.09\n",
            "[49 | 86.65] loss=3.04 avg=3.09\n",
            "[50 | 88.30] loss=3.36 avg=3.10\n",
            "[51 | 89.96] loss=2.39 avg=3.08\n",
            "[52 | 91.62] loss=3.11 avg=3.08\n",
            "[53 | 93.26] loss=3.77 avg=3.10\n",
            "[54 | 94.90] loss=2.48 avg=3.08\n",
            "[55 | 96.54] loss=3.01 avg=3.08\n",
            "[56 | 98.18] loss=2.74 avg=3.07\n",
            "[57 | 99.82] loss=2.65 avg=3.06\n",
            "[58 | 101.45] loss=2.61 avg=3.05\n",
            "[59 | 103.10] loss=3.13 avg=3.05\n",
            "[60 | 104.73] loss=2.74 avg=3.05\n",
            "[61 | 106.36] loss=3.11 avg=3.05\n",
            "[62 | 107.98] loss=3.04 avg=3.05\n",
            "[63 | 109.60] loss=2.86 avg=3.04\n",
            "[64 | 111.23] loss=2.99 avg=3.04\n",
            "[65 | 112.85] loss=3.26 avg=3.05\n",
            "[66 | 114.47] loss=3.67 avg=3.06\n",
            "[67 | 116.09] loss=2.71 avg=3.05\n",
            "[68 | 117.70] loss=2.71 avg=3.05\n",
            "[69 | 119.32] loss=3.18 avg=3.05\n",
            "[70 | 120.94] loss=2.46 avg=3.04\n",
            "[71 | 122.55] loss=2.99 avg=3.04\n",
            "[72 | 124.17] loss=3.51 avg=3.04\n",
            "[73 | 125.78] loss=3.10 avg=3.05\n",
            "[74 | 127.39] loss=2.56 avg=3.04\n",
            "[75 | 129.01] loss=2.66 avg=3.03\n",
            "[76 | 130.62] loss=3.35 avg=3.04\n",
            "[77 | 132.23] loss=3.00 avg=3.03\n",
            "[78 | 133.85] loss=3.38 avg=3.04\n",
            "[79 | 135.46] loss=3.03 avg=3.04\n",
            "[80 | 137.07] loss=2.91 avg=3.04\n",
            "[81 | 138.69] loss=3.06 avg=3.04\n",
            "[82 | 140.31] loss=3.03 avg=3.04\n",
            "[83 | 141.92] loss=2.97 avg=3.04\n",
            "[84 | 143.55] loss=2.49 avg=3.03\n",
            "[85 | 145.16] loss=2.84 avg=3.02\n",
            "[86 | 146.78] loss=2.66 avg=3.02\n",
            "[87 | 148.40] loss=3.45 avg=3.03\n",
            "[88 | 150.01] loss=3.20 avg=3.03\n",
            "[89 | 151.62] loss=3.15 avg=3.03\n",
            "[90 | 153.25] loss=3.30 avg=3.04\n",
            "[91 | 154.86] loss=3.15 avg=3.04\n",
            "[92 | 156.49] loss=3.09 avg=3.04\n",
            "[93 | 158.11] loss=3.28 avg=3.04\n",
            "[94 | 159.73] loss=2.84 avg=3.04\n",
            "[95 | 161.35] loss=3.17 avg=3.04\n",
            "[96 | 162.97] loss=3.36 avg=3.05\n",
            "[97 | 164.60] loss=3.42 avg=3.05\n",
            "[98 | 166.22] loss=2.54 avg=3.04\n",
            "[99 | 167.84] loss=2.42 avg=3.03\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " walking for your life.\"\n",
            "As for her sister, it was clear that her mind was full of the question and answer of all these questions and answers:\n",
            "\"What has been the cause of her being a cripple, and a cripple for ever?\"\n",
            "To which my mother smiled :\n",
            "\"Your brother was a very intelligent and clever man, Mr. Robinson. He told me at another time that your sister did not understand anything about being a cripple, because she was not very clever.\n",
            "That made me feel very much the better of you, as I could have done nothing about it even if I had wanted to.\"\n",
            "It was plain to my mother that to be of any use to a crippled fellow would be nothing in comparison with being of use to you ; as she had seen how easily her niece could be of use to you, in a wise mother's case she saw her chance.\n",
            "\"I will go and take your sister up into the garden,\" she said to myself, with a little smile on her face : \"I am almost so pleased with your niece as to feel that she might not miss out on all her uncle's plans if she should come in at the little inn .\n",
            "Go along to her auntie's house, and find her out .\n",
            "I shall talk to her about it and keep it all to myself . \"\n",
            "She saw I would not leave her auntie's house and come to the little inn, and her sister did not miss out on the whole plan . \"\n",
            "We shall stay there for a season till a man comes along who wants to buy the place for us , but I will stay in my room , and if necessary, at my auntie's as usual , as my cousin will go to town to get a job , and you will stay in the cottage and tell me all your thoughts . \"\n",
            "I waited eagerly to hear that my cousin , the young boy with a head full of money , was not going to town to get a job , and my mother had a kind heart and spirit and a good mind for me and for your niece : she had a heart full of the thought that when the time was the boy might do me the love and kindness I wanted to see him earn, and I would be the better for it .\n",
            "I could not give her up : she was a true woman of twenty-four who made no mistake that I was your brother's brother .\n",
            "I loved her for this .\n",
            "I knew it all right and knew it all in the same way that I loved my sister.\n",
            "But I did not say so to my brother as to the poor old woman , and I did not come to ask her to buy me things and to go off and join my uncle and her auntie in the cottage as my cousin would not have me in her . \"\n",
            "All of a sudden the doorbell beeped , and my uncle sprang up , saying : \" There is no room on that floor , my dear-little girl , and you are in your mother's brother's room . \"\n",
            "\" How dare you ? \"\n",
            "I said indignantly,\n",
            "\" I have got no room . \"\n",
            "Then my uncle put his face to mine and said: \" My dear girl , how many years have I forgotten you ? \"\n",
            "The doorbell again bellowed , and now it was my uncle who rang.\n",
            "My uncle said : \" My dear little girl , you are my cousin's sister; I am your uncle ; I want to come round to-morrow . \"\n",
            "My cousin could not get into the room and go away without me , but rather she would have me come to her and tell her that I wanted to go to town and get a job .\n",
            "She gave me her hand in return for the house as my cousin wanted to join his uncle in the little kitchen of the inn .\n",
            "My uncle went into my cousin's room and sat on the floor on which lay the bed and all her clothes : he was very kind and kind and well-meaning of her .\n",
            "I told her all.\n",
            "I told her that I used to see her niece, and that I used to think how sweet the sight of such a girl would be ; and that I thought I would not have much trouble finding a job in that cottage and doing my cousin's work for her in the little kitchen .\n",
            "I told her I would stay in the room , and I would go and get myself two jobs from a good uncle . \"\n",
            "She said : \" Dear friend , I say to you , I always wished to hear of a fellow like yourself ; it is quite a pity that you do not know the place or the time . \"\n",
            "My uncle was much pleased with her answers ; so glad was he that he promised her the job and the cottage with a view to her taking it for herself , in the little kitchen in the cottage .\n",
            "So I went downstairs to the cottage together , and I set my uncle down at the kitchen and sat down by the fire .\n",
            "My uncle\n",
            "\n",
            "[100 | 195.06] loss=2.56 avg=3.03\n",
            "[101 | 196.70] loss=3.16 avg=3.03\n",
            "[102 | 198.33] loss=3.06 avg=3.03\n",
            "[103 | 199.97] loss=3.67 avg=3.04\n",
            "[104 | 201.62] loss=2.52 avg=3.03\n",
            "[105 | 203.26] loss=2.69 avg=3.03\n",
            "[106 | 204.90] loss=3.26 avg=3.03\n",
            "[107 | 206.54] loss=2.76 avg=3.03\n",
            "[108 | 208.17] loss=2.89 avg=3.02\n",
            "[109 | 209.81] loss=2.99 avg=3.02\n",
            "[110 | 211.45] loss=2.68 avg=3.02\n",
            "[111 | 213.07] loss=3.33 avg=3.02\n",
            "[112 | 214.71] loss=2.56 avg=3.02\n",
            "[113 | 216.34] loss=3.24 avg=3.02\n",
            "[114 | 217.98] loss=3.21 avg=3.02\n",
            "[115 | 219.62] loss=3.76 avg=3.03\n",
            "[116 | 221.25] loss=3.03 avg=3.03\n",
            "[117 | 222.88] loss=3.43 avg=3.04\n",
            "[118 | 224.52] loss=3.18 avg=3.04\n",
            "[119 | 226.15] loss=3.19 avg=3.04\n",
            "[120 | 227.78] loss=2.44 avg=3.03\n",
            "[121 | 229.42] loss=2.88 avg=3.03\n",
            "[122 | 231.05] loss=3.73 avg=3.04\n",
            "[123 | 232.68] loss=2.48 avg=3.03\n",
            "[124 | 234.32] loss=2.54 avg=3.03\n",
            "[125 | 235.95] loss=2.89 avg=3.02\n",
            "[126 | 237.58] loss=2.81 avg=3.02\n",
            "[127 | 239.21] loss=3.86 avg=3.03\n",
            "[128 | 240.85] loss=2.84 avg=3.03\n",
            "[129 | 242.48] loss=3.69 avg=3.04\n",
            "[130 | 244.12] loss=3.48 avg=3.05\n",
            "[131 | 245.75] loss=2.84 avg=3.04\n",
            "[132 | 247.39] loss=2.71 avg=3.04\n",
            "[133 | 249.02] loss=2.78 avg=3.03\n",
            "[134 | 250.65] loss=3.00 avg=3.03\n",
            "[135 | 252.28] loss=2.71 avg=3.03\n",
            "[136 | 253.92] loss=2.92 avg=3.03\n",
            "[137 | 255.56] loss=2.86 avg=3.03\n",
            "[138 | 257.18] loss=3.50 avg=3.03\n",
            "[139 | 258.82] loss=3.17 avg=3.03\n",
            "[140 | 260.45] loss=3.48 avg=3.04\n",
            "[141 | 262.09] loss=3.28 avg=3.04\n",
            "[142 | 263.73] loss=2.96 avg=3.04\n",
            "[143 | 265.36] loss=3.08 avg=3.04\n",
            "[144 | 267.00] loss=3.00 avg=3.04\n",
            "[145 | 268.63] loss=3.10 avg=3.04\n",
            "[146 | 270.27] loss=2.76 avg=3.04\n",
            "[147 | 271.91] loss=2.47 avg=3.03\n",
            "[148 | 273.54] loss=3.10 avg=3.03\n",
            "[149 | 275.18] loss=2.41 avg=3.02\n",
            "[150 | 276.82] loss=2.81 avg=3.02\n",
            "[151 | 278.46] loss=2.55 avg=3.02\n",
            "[152 | 280.10] loss=3.97 avg=3.03\n",
            "[153 | 281.74] loss=2.85 avg=3.03\n",
            "[154 | 283.38] loss=2.96 avg=3.03\n",
            "[155 | 285.02] loss=3.06 avg=3.03\n",
            "[156 | 286.66] loss=2.46 avg=3.02\n",
            "[157 | 288.30] loss=2.51 avg=3.01\n",
            "[158 | 289.93] loss=2.50 avg=3.01\n",
            "[159 | 291.57] loss=2.64 avg=3.00\n",
            "[160 | 293.21] loss=3.65 avg=3.01\n",
            "[161 | 294.85] loss=2.97 avg=3.01\n",
            "[162 | 296.49] loss=2.87 avg=3.01\n",
            "[163 | 298.13] loss=2.86 avg=3.01\n",
            "[164 | 299.77] loss=3.21 avg=3.01\n",
            "[165 | 301.40] loss=3.29 avg=3.01\n",
            "[166 | 303.04] loss=2.95 avg=3.01\n",
            "[167 | 304.68] loss=3.67 avg=3.02\n",
            "[168 | 306.31] loss=3.29 avg=3.02\n",
            "[169 | 307.95] loss=3.26 avg=3.02\n",
            "[170 | 309.58] loss=3.03 avg=3.02\n",
            "[171 | 311.20] loss=3.11 avg=3.03\n",
            "[172 | 312.84] loss=3.44 avg=3.03\n",
            "[173 | 314.48] loss=2.28 avg=3.02\n",
            "[174 | 316.12] loss=3.42 avg=3.03\n",
            "[175 | 317.76] loss=3.37 avg=3.03\n",
            "[176 | 319.39] loss=3.11 avg=3.03\n",
            "[177 | 321.03] loss=3.22 avg=3.03\n",
            "[178 | 322.67] loss=2.63 avg=3.03\n",
            "[179 | 324.31] loss=2.84 avg=3.03\n",
            "[180 | 325.95] loss=3.33 avg=3.03\n",
            "[181 | 327.59] loss=2.30 avg=3.02\n",
            "[182 | 329.23] loss=3.07 avg=3.02\n",
            "[183 | 330.86] loss=3.40 avg=3.03\n",
            "[184 | 332.51] loss=2.75 avg=3.02\n",
            "[185 | 334.15] loss=2.92 avg=3.02\n",
            "[186 | 335.79] loss=2.45 avg=3.02\n",
            "[187 | 337.43] loss=3.68 avg=3.02\n",
            "[188 | 339.07] loss=3.55 avg=3.03\n",
            "[189 | 340.70] loss=3.15 avg=3.03\n",
            "[190 | 342.34] loss=3.18 avg=3.03\n",
            "[191 | 343.98] loss=2.80 avg=3.03\n",
            "[192 | 345.62] loss=3.70 avg=3.04\n",
            "[193 | 347.25] loss=3.37 avg=3.04\n",
            "[194 | 348.89] loss=2.75 avg=3.04\n",
            "[195 | 350.52] loss=2.82 avg=3.04\n",
            "[196 | 352.17] loss=3.19 avg=3.04\n",
            "[197 | 353.80] loss=3.06 avg=3.04\n",
            "[198 | 355.45] loss=3.98 avg=3.05\n",
            "[199 | 357.08] loss=2.77 avg=3.05\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " sea , and I found myself in the most extraordinary place .\n",
            "I found myself at a rock in a country , which I had driven down with all my speed at the very beginning of my journey .\n",
            "I looked out of the windows , and saw myself at the western point of the coast , surrounded by a large hillside , and with the greatest variety of trees , both with and without leaves ; and I heard the groans of the sea as I passed it .\n",
            "The land seemed to be full , and my thoughts were not wholly away from the land .\n",
            "At my arrival the shore was still , and it was impossible to find another place to lie down ; so I found it well to rest my weary body till the season of my journey came , and then to start , and to look up and see where I had come to , or to look back and see where I must go .\n",
            "I found that my heart was in the right place ; indeed , from my first step I could not help thinking of the glory and the greatness of a good place .\n",
            "At length , when I had nearly been to the western end of the coast , I perceived myself in a vast square like a giant .\n",
            "And so I started with all my speed , and walked with my heart and my soul in my breast , while those who were nearest to me were nearer still ; while those farther off might lay farther back and lay farther forward , and still they might lie down in their comfort and find their places in the forest , as I did , and sleep soundly in that place .\n",
            "And it was easy for me , but many a time I kept wandering till I found myself where I should sleep .\n",
            "So far had I gone that it was my last good night there , even if I might sleep soundly there .\n",
            "And I thought of the land and the things lying there ; but there was no place for me there , at least I knew it -- and I did not care to be in a land where I could not be in my true home .\n",
            "There were the trees that stood on the western side of the beach , to which the breeze had carried me ; those , too , were very pleasant .\n",
            "I could see them , but I was so far from them , and could look away at nothing so near as the sea -- so I was obliged to walk down and lay down in the wood and in the woods , while the sea came about me and took me along .\n",
            "But the sea , when she came , was not so strong as I had feared .\n",
            "As I walked along the beach I could see from where I lay on his left a stream running down over that place to which I was so long in the forest , that I almost forgot what I said ; but , though there was nothing to think of but this , I was at a loss of what to say , and thought it best to tell her only by going up the stream that led to the beach and looking for an old man .\n",
            "When I had gone as far as the stream, I found him , to whom I was so fond , and who would lay me all ready for my farewell journey , as he did me with words and his warm smile that no one can betray to me .\n",
            "He took me into a little house that had been built upon the beach ; and in it was a long table which was not so large as my one , but as I was the guest he had brought along , I knew exactly what it had been used for ; and it turned out to be very large indeed , and I found it fit for one man as well as for a wife and a child !<|endoftext|>In a video first broadcast in 2013, the president of the Islamic Supreme Council of Nigeria (ISCN) Sheikh Abdul Razak Ali Bhuiyan warned U.S. troops that they could face deadly retribution if they attempted to harm the Islamic State (ISIS).\n",
            "\n",
            "ISCN President Sheikh Abdul Razak Ali Bhuiyan has warned that Islamic State and its American supporters will be able to use nuclear weapons, while warning U.S. troops that they faced severe retaliation if they attempted to harm ISIS in the Middle East, according to the New York Times on Thursday.\n",
            "\n",
            "According to the Times, the Islamic State has already started using nuclear weapons on a wider basis.\n",
            "\n",
            "As an example, the Washington Post reported that the United States is working on a plan to develop an intercontinental ballistic missile capable of hitting Russia. The plans call for the United States to launch two of such missiles at Russia at the same time for a total of 14 of such weapons to strike each other. These missiles have not been launched yet but are planned over the next six years.\n",
            "\n",
            "Islamic State has shown an interest in intercontinental ballistic missiles and could become more ambitious about developing them. For example, in October, Islamic State published a video in which fighters and their weapons are deployed and ready for launch against the U.S. The video showed its fighters and heavy weapons deployed near the border with Russia -- a move\n",
            "\n",
            "[200 | 382.12] loss=2.78 avg=3.04\n",
            "[201 | 383.76] loss=2.66 avg=3.04\n",
            "[202 | 385.40] loss=3.73 avg=3.05\n",
            "[203 | 387.04] loss=2.43 avg=3.04\n",
            "[204 | 388.68] loss=2.68 avg=3.03\n",
            "[205 | 390.30] loss=2.99 avg=3.03\n",
            "[206 | 391.93] loss=3.00 avg=3.03\n",
            "[207 | 393.57] loss=2.75 avg=3.03\n",
            "[208 | 395.20] loss=3.33 avg=3.03\n",
            "[209 | 396.84] loss=3.29 avg=3.04\n",
            "[210 | 398.46] loss=2.87 avg=3.04\n",
            "[211 | 400.09] loss=2.88 avg=3.03\n",
            "[212 | 401.73] loss=3.22 avg=3.04\n",
            "[213 | 403.37] loss=3.45 avg=3.04\n",
            "[214 | 405.01] loss=2.81 avg=3.04\n",
            "[215 | 406.65] loss=2.56 avg=3.03\n",
            "[216 | 408.28] loss=3.34 avg=3.04\n",
            "[217 | 409.91] loss=3.13 avg=3.04\n",
            "[218 | 411.55] loss=2.34 avg=3.03\n",
            "[219 | 413.18] loss=2.97 avg=3.03\n",
            "[220 | 414.82] loss=3.36 avg=3.03\n",
            "[221 | 416.45] loss=3.42 avg=3.04\n",
            "[222 | 418.09] loss=3.26 avg=3.04\n",
            "[223 | 419.72] loss=3.01 avg=3.04\n",
            "[224 | 421.36] loss=2.89 avg=3.04\n",
            "[225 | 423.00] loss=2.52 avg=3.03\n",
            "[226 | 424.63] loss=3.00 avg=3.03\n",
            "[227 | 426.26] loss=2.53 avg=3.02\n",
            "[228 | 427.90] loss=3.29 avg=3.03\n",
            "[229 | 429.54] loss=2.91 avg=3.03\n",
            "[230 | 431.18] loss=3.22 avg=3.03\n",
            "[231 | 432.81] loss=3.30 avg=3.03\n",
            "[232 | 434.45] loss=2.38 avg=3.02\n",
            "[233 | 436.09] loss=3.33 avg=3.03\n",
            "[234 | 437.72] loss=2.96 avg=3.03\n",
            "[235 | 439.36] loss=3.24 avg=3.03\n",
            "[236 | 441.00] loss=3.00 avg=3.03\n",
            "[237 | 442.64] loss=3.70 avg=3.04\n",
            "[238 | 444.28] loss=2.63 avg=3.03\n",
            "[239 | 445.92] loss=3.12 avg=3.03\n",
            "[240 | 447.56] loss=2.74 avg=3.03\n",
            "[241 | 449.20] loss=2.91 avg=3.03\n",
            "[242 | 450.83] loss=3.38 avg=3.03\n",
            "[243 | 452.47] loss=3.22 avg=3.03\n",
            "[244 | 454.11] loss=2.90 avg=3.03\n",
            "[245 | 455.75] loss=3.16 avg=3.03\n",
            "[246 | 457.39] loss=2.51 avg=3.03\n",
            "[247 | 459.03] loss=3.09 avg=3.03\n",
            "[248 | 460.67] loss=3.12 avg=3.03\n",
            "[249 | 462.31] loss=2.60 avg=3.03\n",
            "[250 | 463.94] loss=2.70 avg=3.02\n",
            "[251 | 465.58] loss=2.95 avg=3.02\n",
            "[252 | 467.22] loss=3.03 avg=3.02\n",
            "[253 | 468.86] loss=2.97 avg=3.02\n",
            "[254 | 470.50] loss=3.28 avg=3.02\n",
            "[255 | 472.14] loss=3.12 avg=3.02\n",
            "[256 | 473.78] loss=2.68 avg=3.02\n",
            "[257 | 475.42] loss=3.70 avg=3.03\n",
            "[258 | 477.06] loss=2.55 avg=3.02\n",
            "[259 | 478.70] loss=2.60 avg=3.02\n",
            "[260 | 480.34] loss=2.82 avg=3.02\n",
            "[261 | 481.98] loss=2.71 avg=3.01\n",
            "[262 | 483.62] loss=3.40 avg=3.02\n",
            "[263 | 485.26] loss=2.45 avg=3.01\n",
            "[264 | 486.90] loss=2.98 avg=3.01\n",
            "[265 | 488.53] loss=2.67 avg=3.01\n",
            "[266 | 490.17] loss=2.55 avg=3.00\n",
            "[267 | 491.82] loss=2.90 avg=3.00\n",
            "[268 | 493.46] loss=3.34 avg=3.00\n",
            "[269 | 495.10] loss=3.19 avg=3.01\n",
            "[270 | 496.74] loss=2.55 avg=3.00\n",
            "[271 | 498.38] loss=2.98 avg=3.00\n",
            "[272 | 500.02] loss=2.97 avg=3.00\n",
            "[273 | 501.66] loss=3.77 avg=3.01\n",
            "[274 | 503.30] loss=3.03 avg=3.01\n",
            "[275 | 504.94] loss=2.74 avg=3.01\n",
            "[276 | 506.58] loss=3.60 avg=3.01\n",
            "[277 | 508.22] loss=3.01 avg=3.01\n",
            "[278 | 509.86] loss=2.90 avg=3.01\n",
            "[279 | 511.50] loss=2.88 avg=3.01\n",
            "[280 | 513.13] loss=3.47 avg=3.02\n",
            "[281 | 514.78] loss=2.81 avg=3.01\n",
            "[282 | 516.41] loss=2.37 avg=3.01\n",
            "[283 | 518.05] loss=2.30 avg=3.00\n",
            "[284 | 519.69] loss=2.68 avg=3.00\n",
            "[285 | 521.33] loss=2.96 avg=3.00\n",
            "[286 | 522.97] loss=3.43 avg=3.00\n",
            "[287 | 524.61] loss=3.31 avg=3.00\n",
            "[288 | 526.25] loss=2.53 avg=3.00\n",
            "[289 | 527.89] loss=3.36 avg=3.00\n",
            "[290 | 529.53] loss=2.90 avg=3.00\n",
            "[291 | 531.17] loss=3.22 avg=3.00\n",
            "[292 | 532.81] loss=3.00 avg=3.00\n",
            "[293 | 534.44] loss=3.24 avg=3.01\n",
            "[294 | 536.09] loss=2.62 avg=3.00\n",
            "[295 | 537.73] loss=2.82 avg=3.00\n",
            "[296 | 539.37] loss=3.35 avg=3.00\n",
            "[297 | 541.00] loss=2.97 avg=3.00\n",
            "[298 | 542.64] loss=2.49 avg=3.00\n",
            "[299 | 544.28] loss=2.56 avg=2.99\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " .\n",
            "Sparks fell from it and the whole air was stirred up before them .\n",
            "A thunderbolt struck them all so hard and so quickly that the sparks went straight into Sam 's hands like hail and he fell backward .\n",
            "A large and strong hand was at his side and stopped him down .\n",
            "Sam looked up to see a shadow standing over him .\n",
            "It had an even grayer head and pointed to the sky and a tail of golden dust that waved about , then disappeared again .\n",
            "The thunderbolt went past the tree and struck another large one near it .\n",
            "Suddenly this one stopped .\n",
            "A tremendous explosion took place .\n",
            "The tree trembled and a great many limbs fell as if thrown on top of one another .\n",
            "Then there was a strong crackle of thunder in the air like the ringing of a bell .\n",
            "Everything exploded -- the tree , the town and everything in the surrounding region .\n",
            "It was only a flash .\n",
            "Sam was lying down on a low tree close to a river with his face in the snow .\n",
            "The storm passed away , the thunder ceased , and the light was gradually broken and dimmed by the clouds .\n",
            "He saw that a great mass of air was coming from north , and that from the same place he was standing .\n",
            "He looked up over it and thought : \"That 's a storm of winds . \"\n",
            "Then his face fell again .\n",
            "He thought that the wind was gathering strength from the north , and that it was coming from the east .\n",
            "The storm was now approaching .\n",
            "The people were all on their way to the west .\n",
            "They had left the town in the early morning and went straight back .\n",
            "All of the inhabitants , and all the inhabitants ' servants , had gone to a distance of about two hundred paces from the centre of the town .\n",
            "All were standing to one side of the stream .\n",
            "They had come straight over ; they had left the town and were now going straight home in the direction in which they had come .\n",
            "They were all of them on the right and all of them on the left .\n",
            "Then Sam was near the river and knew that he had to keep coming with the wind to the east until he passed another strong storm , of wind , that was going north , and that was coming north of him and then he knew that he must try to follow the wind that was coming from the west ; for he wanted to get to the town as soon as possible .\n",
            "All those he was following were about two hundred paces from him .\n",
            "Thus , all those he had left on the way towards the west and none who had not gone straight home , had entered the town , and since then there had been two powerful and dangerous storms .\n",
            "All those whom he followed were of the same age as him and all were of a very strong mind .\n",
            "It was necessary for him to keep him , for one could never get a wife except by strength of character .\n",
            "So Sam got on the shoulders of the storm , and he did all the more in his mind and heart to keep the storm at its source .\n",
            "At the beginning of the fourth and fifth days , the storm was blowing northward in a northerly direction toward the town .\n",
            "The sky was clouded up with clouds , and the snow was falling from it slowly ; so , as soon as the wind came out of its channel into the stream , its ripples and cracks were so great that they made the whole river look quite like a carpet of snow , though not so very much so .\n",
            "At the time of the storm , all that snow had been melted from the earth and was being turned to dust by the wind .\n",
            "The snow came into sight suddenly and it gave Sam the feeling that he had lost the right side of his head .\n",
            "The storm did not stop any further until it reached the village of La Bala near the town of Elba .\n",
            "The place was situated just up to the river and it was at that instant the storm stopped .\n",
            "All except one and he was the last one to go .\n",
            "The storm began to move farther on toward the south bank of the river and then the wind began to blow against the storm and there was a great howl , as if it were going to fall upon us , and all that came into the way were very hard to control .\n",
            "It was the first day of the new year and the weather was fair and sunny .\n",
            "The wind was very strong and the rain was heavy , and the snow was falling very slowly . However , with great difficulty everything went forward and there was no longer a little danger .\n",
            "The people went straight into the town -- to the house of the very same name that Sam was walking past -- in order to have some food to eat and drink to have some fresh air to breathe .\n",
            "The people remained to sleep and were quite calm .\n",
            "But , on the third day of the month , the very same weather became worse .\n",
            "It was very cold .\n",
            "The people seemed to\n",
            "\n",
            "[300 | 569.38] loss=2.42 avg=2.99\n",
            "[301 | 571.00] loss=2.50 avg=2.98\n",
            "[302 | 572.64] loss=3.33 avg=2.99\n",
            "[303 | 574.28] loss=3.20 avg=2.99\n",
            "[304 | 575.92] loss=3.34 avg=2.99\n",
            "[305 | 577.56] loss=3.47 avg=3.00\n",
            "[306 | 579.19] loss=3.14 avg=3.00\n",
            "[307 | 580.82] loss=3.10 avg=3.00\n",
            "[308 | 582.46] loss=2.93 avg=3.00\n",
            "[309 | 584.09] loss=2.78 avg=3.00\n",
            "[310 | 585.72] loss=3.06 avg=3.00\n",
            "[311 | 587.36] loss=2.76 avg=2.99\n",
            "[312 | 588.99] loss=3.66 avg=3.00\n",
            "[313 | 590.63] loss=3.16 avg=3.00\n",
            "[314 | 592.26] loss=3.54 avg=3.01\n",
            "[315 | 593.90] loss=3.12 avg=3.01\n",
            "[316 | 595.53] loss=3.16 avg=3.01\n",
            "[317 | 597.17] loss=3.43 avg=3.02\n",
            "[318 | 598.80] loss=2.60 avg=3.01\n",
            "[319 | 600.43] loss=2.80 avg=3.01\n",
            "[320 | 602.06] loss=3.49 avg=3.01\n",
            "[321 | 603.70] loss=2.94 avg=3.01\n",
            "[322 | 605.33] loss=3.52 avg=3.02\n",
            "[323 | 606.96] loss=3.14 avg=3.02\n",
            "[324 | 608.59] loss=2.85 avg=3.02\n",
            "[325 | 610.22] loss=3.48 avg=3.02\n",
            "[326 | 611.85] loss=2.67 avg=3.02\n",
            "[327 | 613.49] loss=2.70 avg=3.02\n",
            "[328 | 615.12] loss=2.64 avg=3.01\n",
            "[329 | 616.75] loss=2.53 avg=3.01\n",
            "[330 | 618.38] loss=3.59 avg=3.01\n",
            "[331 | 620.02] loss=3.21 avg=3.01\n",
            "[332 | 621.65] loss=3.03 avg=3.02\n",
            "[333 | 623.28] loss=2.95 avg=3.01\n",
            "[334 | 624.90] loss=2.65 avg=3.01\n",
            "[335 | 626.54] loss=2.62 avg=3.01\n",
            "[336 | 628.17] loss=2.50 avg=3.00\n",
            "[337 | 629.80] loss=3.41 avg=3.01\n",
            "[338 | 631.43] loss=3.25 avg=3.01\n",
            "[339 | 633.05] loss=2.94 avg=3.01\n",
            "[340 | 634.69] loss=3.23 avg=3.01\n",
            "[341 | 636.31] loss=3.11 avg=3.01\n",
            "[342 | 637.94] loss=2.42 avg=3.00\n",
            "[343 | 639.57] loss=3.41 avg=3.01\n",
            "[344 | 641.21] loss=2.82 avg=3.01\n",
            "[345 | 642.84] loss=3.07 avg=3.01\n",
            "[346 | 644.46] loss=2.93 avg=3.01\n",
            "[347 | 646.09] loss=2.58 avg=3.00\n",
            "[348 | 647.72] loss=3.28 avg=3.01\n",
            "[349 | 649.35] loss=3.27 avg=3.01\n",
            "[350 | 650.98] loss=3.21 avg=3.01\n",
            "[351 | 652.60] loss=2.53 avg=3.01\n",
            "[352 | 654.24] loss=3.90 avg=3.01\n",
            "[353 | 655.86] loss=3.39 avg=3.02\n",
            "[354 | 657.48] loss=3.46 avg=3.02\n",
            "[355 | 659.11] loss=2.82 avg=3.02\n",
            "[356 | 660.74] loss=2.60 avg=3.02\n",
            "[357 | 662.37] loss=3.48 avg=3.02\n",
            "[358 | 664.01] loss=2.28 avg=3.01\n",
            "[359 | 665.64] loss=2.77 avg=3.01\n",
            "[360 | 667.28] loss=3.17 avg=3.01\n",
            "[361 | 668.91] loss=3.36 avg=3.02\n",
            "[362 | 670.54] loss=2.86 avg=3.01\n",
            "[363 | 672.18] loss=3.11 avg=3.02\n",
            "[364 | 673.81] loss=2.91 avg=3.01\n",
            "[365 | 675.44] loss=3.36 avg=3.02\n",
            "[366 | 677.08] loss=2.70 avg=3.01\n",
            "[367 | 678.71] loss=2.72 avg=3.01\n",
            "[368 | 680.34] loss=2.65 avg=3.01\n",
            "[369 | 681.98] loss=3.28 avg=3.01\n",
            "[370 | 683.62] loss=3.00 avg=3.01\n",
            "[371 | 685.25] loss=3.08 avg=3.01\n",
            "[372 | 686.88] loss=2.91 avg=3.01\n",
            "[373 | 688.51] loss=2.81 avg=3.01\n",
            "[374 | 690.14] loss=3.17 avg=3.01\n",
            "[375 | 691.77] loss=3.04 avg=3.01\n",
            "[376 | 693.41] loss=2.35 avg=3.00\n",
            "[377 | 695.04] loss=3.33 avg=3.01\n",
            "[378 | 696.67] loss=2.62 avg=3.00\n",
            "[379 | 698.31] loss=3.16 avg=3.00\n",
            "[380 | 699.94] loss=2.83 avg=3.00\n",
            "[381 | 701.57] loss=2.60 avg=3.00\n",
            "[382 | 703.19] loss=2.75 avg=3.00\n",
            "[383 | 704.83] loss=3.28 avg=3.00\n",
            "[384 | 706.45] loss=2.82 avg=3.00\n",
            "[385 | 708.08] loss=2.69 avg=2.99\n",
            "[386 | 709.72] loss=2.70 avg=2.99\n",
            "[387 | 711.35] loss=2.50 avg=2.99\n",
            "[388 | 712.97] loss=2.98 avg=2.99\n",
            "[389 | 714.61] loss=2.60 avg=2.98\n",
            "[390 | 716.23] loss=3.11 avg=2.98\n",
            "[391 | 717.87] loss=2.93 avg=2.98\n",
            "[392 | 719.50] loss=3.17 avg=2.98\n",
            "[393 | 721.13] loss=2.79 avg=2.98\n",
            "[394 | 722.76] loss=2.96 avg=2.98\n",
            "[395 | 724.40] loss=2.36 avg=2.98\n",
            "[396 | 726.03] loss=3.47 avg=2.98\n",
            "[397 | 727.66] loss=2.39 avg=2.97\n",
            "[398 | 729.29] loss=3.13 avg=2.98\n",
            "[399 | 730.92] loss=2.81 avg=2.97\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " among that which is not mine.\" He replied, \"But I do want to have a word. You shall not find another in whom you may look for happiness.\"\n",
            "This reply was in reply to the last word of the answer of Ptolemy to him who had promised to have such a word from him with so much kindness and kindness in his hands but having lost all the time for his promise.\n",
            "Now Ptolemy had promised him a letter that was bound to the king , to be delivered by sea , but the letter never arrived , and he and many thousands of others saw that and rejoiced greatly at the prospect of a happy present from the King of the West , which is called Ptolemy Philopator .\n",
            "CHAPTER XVII.\n",
            "At the same time in his palace in Troyes he began to talk by night : and then he was heard telling his stories to those who came to listen to them .\n",
            "A king of the Assyrians , a prince of Parthia , a prince of the Assyrians who had left Persia for the King of Persia , and was not yet married , was lying on a chair in the room which he had left for himself in the palace of Tegea the son of Eudus , by name .\n",
            "The king sat beside him and the prince , whose wife was beautiful and fairer than anything else of all the kings in the world , sat nearest the king .\n",
            "An hour or two passed , and then the king called the prince who heard his tale by sleep : \" The King of the West , who is now at the king's palace , has promised to come with you tomorrow morning . \"\n",
            "The prince was startled , and said , `` The king has said the same day , but he is not come so early as you promised .\n",
            "Do you , indeed , not know him but , as the wise have said , by night ? ''\n",
            "`` You are mistaken , '' answered the king , `` he comes at night . ''\n",
            "At that the prince smiled bitterly , and said , `` No , the king comes not at midnight . ''\n",
            "In which words the king was silent , and the prince looked about him .\n",
            "Suddenly out of the king 's balcony he heard a voice , and in a moment there stood at the balcony , like a huge lion , a man , and the name of the man was Erech .\n",
            "`` Who am I ? ''\n",
            "said Erech .\n",
            "`` I am the King who has promised you the letter which you asked for .\n",
            "I am the King of the East , in Parthia .\n",
            "I came from the palace of Tegea -- the son of Eudus , by name -- who was my father before he married my mother to this one .\n",
            "I left him at the palace , and in the palace of my father he was waiting , for I had found him to whom it was not strange to go -- and that the man who was waiting by night was only a very old man by birth and age and size and height who had come there to get one .\n",
            "I thought -- not that I could believe it -- that for the king so late in the day there was no king , for the King of the West was still alive .\n",
            "I had thought that I never could tell you the truth , and you could only look at me and pray for the letter in the night and believe I had the letter , and you have believed it when you saw me standing at the balcony .\n",
            "There is no hope of a happy present , if we go here when our hopes are gone , but it is in the evening ; and I am so glad now that you are come .\n",
            "I did speak to you yesterday , and now the King of the West is the one whom I shall tell you . ''\n",
            "And immediately the prince was silent , and said to his father , `` Who is Erech ? ''\n",
            "`` Are you the son of the King of the West , who is a prince of Parthia ? ''\n",
            "`` I am . ''\n",
            "The prince began , `` Who art thou ? ''\n",
            "The father smiled , and answered , `` As you may see me sitting on one of the chairs of the balcony which you have just seen , I am the King of the East , in Parthia . ''\n",
            "As soon as the prince knew his father in whose palace he stood and in whose palace he spoke , he spoke : `` Who am I , then ?\n",
            "I am the King of the West , in Parthia .\n",
            "I do not know when I shall have a husband , and I do not know who is my father , and I do not believe , for the King of the West is dead , that there is anybody in the world who could help it .\n",
            "But I know that there is a king who will have pleasure with me , even to the extent that , for me to take up his sword and kill him for your sake , will become a sad and melancholy event .\n",
            "In my absence he will come hither and marry\n",
            "\n",
            "[400 | 755.74] loss=2.65 avg=2.97\n",
            "[401 | 757.38] loss=3.29 avg=2.97\n",
            "[402 | 759.01] loss=2.73 avg=2.97\n",
            "[403 | 760.63] loss=3.01 avg=2.97\n",
            "[404 | 762.26] loss=2.68 avg=2.97\n",
            "[405 | 763.89] loss=2.81 avg=2.97\n",
            "[406 | 765.53] loss=2.37 avg=2.96\n",
            "[407 | 767.16] loss=2.91 avg=2.96\n",
            "[408 | 768.78] loss=2.98 avg=2.96\n",
            "[409 | 770.41] loss=2.61 avg=2.96\n",
            "[410 | 772.04] loss=2.86 avg=2.96\n",
            "[411 | 773.67] loss=2.66 avg=2.95\n",
            "[412 | 775.30] loss=3.45 avg=2.96\n",
            "[413 | 776.91] loss=3.18 avg=2.96\n",
            "[414 | 778.54] loss=3.47 avg=2.97\n",
            "[415 | 780.17] loss=2.85 avg=2.97\n",
            "[416 | 781.81] loss=2.69 avg=2.96\n",
            "[417 | 783.43] loss=2.53 avg=2.96\n",
            "[418 | 785.07] loss=2.84 avg=2.96\n",
            "[419 | 786.69] loss=3.52 avg=2.96\n",
            "[420 | 788.32] loss=2.99 avg=2.96\n",
            "[421 | 789.95] loss=2.64 avg=2.96\n",
            "[422 | 791.58] loss=3.23 avg=2.96\n",
            "[423 | 793.21] loss=2.58 avg=2.96\n",
            "[424 | 794.84] loss=2.91 avg=2.96\n",
            "[425 | 796.47] loss=2.75 avg=2.96\n",
            "[426 | 798.10] loss=2.77 avg=2.95\n",
            "[427 | 799.73] loss=2.21 avg=2.95\n",
            "[428 | 801.35] loss=3.40 avg=2.95\n",
            "[429 | 802.98] loss=3.17 avg=2.95\n",
            "[430 | 804.61] loss=2.91 avg=2.95\n",
            "[431 | 806.25] loss=2.47 avg=2.95\n",
            "[432 | 807.88] loss=3.30 avg=2.95\n",
            "[433 | 809.51] loss=3.42 avg=2.96\n",
            "[434 | 811.14] loss=3.68 avg=2.96\n",
            "[435 | 812.75] loss=2.65 avg=2.96\n",
            "[436 | 814.39] loss=3.31 avg=2.96\n",
            "[437 | 816.02] loss=2.56 avg=2.96\n",
            "[438 | 817.65] loss=3.15 avg=2.96\n",
            "[439 | 819.28] loss=2.47 avg=2.96\n",
            "[440 | 820.91] loss=2.55 avg=2.95\n",
            "[441 | 822.53] loss=2.63 avg=2.95\n",
            "[442 | 824.16] loss=3.13 avg=2.95\n",
            "[443 | 825.79] loss=3.00 avg=2.95\n",
            "[444 | 827.42] loss=3.19 avg=2.95\n",
            "[445 | 829.05] loss=3.46 avg=2.96\n",
            "[446 | 830.68] loss=3.52 avg=2.96\n",
            "[447 | 832.31] loss=3.28 avg=2.97\n",
            "[448 | 833.94] loss=2.99 avg=2.97\n",
            "[449 | 835.58] loss=2.84 avg=2.97\n",
            "[450 | 837.20] loss=2.77 avg=2.97\n",
            "[451 | 838.83] loss=3.22 avg=2.97\n",
            "[452 | 840.46] loss=2.68 avg=2.96\n",
            "[453 | 842.10] loss=2.28 avg=2.96\n",
            "[454 | 843.74] loss=2.64 avg=2.95\n",
            "[455 | 845.37] loss=2.65 avg=2.95\n",
            "[456 | 847.00] loss=2.86 avg=2.95\n",
            "[457 | 848.64] loss=3.06 avg=2.95\n",
            "[458 | 850.26] loss=2.54 avg=2.95\n",
            "[459 | 851.90] loss=3.16 avg=2.95\n",
            "[460 | 853.53] loss=3.20 avg=2.95\n",
            "[461 | 855.15] loss=2.85 avg=2.95\n",
            "[462 | 856.78] loss=3.33 avg=2.95\n",
            "[463 | 858.41] loss=3.27 avg=2.96\n",
            "[464 | 860.05] loss=2.88 avg=2.96\n",
            "[465 | 861.68] loss=3.39 avg=2.96\n",
            "[466 | 863.31] loss=2.75 avg=2.96\n",
            "[467 | 864.93] loss=2.84 avg=2.96\n",
            "[468 | 866.56] loss=3.45 avg=2.96\n",
            "[469 | 868.19] loss=2.83 avg=2.96\n",
            "[470 | 869.83] loss=3.06 avg=2.96\n",
            "[471 | 871.46] loss=2.74 avg=2.96\n",
            "[472 | 873.09] loss=3.01 avg=2.96\n",
            "[473 | 874.73] loss=2.84 avg=2.96\n",
            "[474 | 876.36] loss=3.28 avg=2.96\n",
            "[475 | 877.99] loss=2.86 avg=2.96\n",
            "[476 | 879.63] loss=2.66 avg=2.96\n",
            "[477 | 881.26] loss=3.10 avg=2.96\n",
            "[478 | 882.89] loss=2.32 avg=2.95\n",
            "[479 | 884.53] loss=3.18 avg=2.96\n",
            "[480 | 886.16] loss=3.26 avg=2.96\n",
            "[481 | 887.79] loss=3.48 avg=2.96\n",
            "[482 | 889.43] loss=3.30 avg=2.97\n",
            "[483 | 891.06] loss=2.61 avg=2.96\n",
            "[484 | 892.69] loss=2.94 avg=2.96\n",
            "[485 | 894.32] loss=2.32 avg=2.96\n",
            "[486 | 895.95] loss=2.51 avg=2.95\n",
            "[487 | 897.59] loss=2.92 avg=2.95\n",
            "[488 | 899.22] loss=2.28 avg=2.95\n",
            "[489 | 900.85] loss=3.52 avg=2.95\n",
            "[490 | 902.49] loss=3.16 avg=2.95\n",
            "[491 | 904.12] loss=3.04 avg=2.95\n",
            "[492 | 905.75] loss=2.91 avg=2.95\n",
            "[493 | 907.38] loss=3.12 avg=2.96\n",
            "[494 | 909.01] loss=2.50 avg=2.95\n",
            "[495 | 910.64] loss=1.98 avg=2.94\n",
            "[496 | 912.28] loss=3.33 avg=2.95\n",
            "[497 | 913.91] loss=3.04 avg=2.95\n",
            "[498 | 915.54] loss=2.97 avg=2.95\n",
            "[499 | 917.17] loss=3.36 avg=2.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " as a new generation of Americans.\"\n",
            "The story's hero was, of course , Benjamin Moore , who died in 1837 after he was captured for treason .\n",
            "In his will Moore had written , \" Behold this little child , the first-born of all children; He shall be my heir and be called the king 's son because he is the first of the line that shall make an end of all the line that is .\n",
            "\" If thou wilt marry me thou shalt receive my hand and thou shalt have my sword .\n",
            "If thou wilt accept my hand , thou shalt be my wife .\n",
            "I have given him the sword .\n",
            "If a man takes this sword thy love shall be safe and the same day shalt thou go to thy mistress ; but if she has two of the sword , thou shalt not go to her .\n",
            "Therefore take two of the sword for me .\n",
            "And I will fight to see the kingdom of thine inheritance which I have . '\n",
            "And so he took the sword for her when he came from prison , but he killed her , for it was her own which had killed him .\n",
            "She had done nothing to him but to tell the truth , and the last thing had passed in two days . '\n",
            "And this was what the child said at the age when she told it .\n",
            "For as I have told you , to the kingdom of his inheritance .\n",
            "This is the story of his father .\n",
            "That is why I shall tell you the story of mine own .\n",
            "The history of my race , it is true ; the descendants of Abraham , the first man in the world .\n",
            "That was the story of my great-great-great-grandfather ; he came from a great-great-grandfather that was the father of my own great-great-great-grandfather .\n",
            "Now after him the line was cut through and through until this great-grandfather called Jacob , the son of Zechariah .\n",
            "And in those days there came a man named Isaiah the prophet .\n",
            "And while his own line was cut and cut through , it came to pass that the people were very angry and asked him , \" What is this that he hath spoken of ? '\n",
            "`` All this is my people that God did take away that have not a wife 's hand , '' Isaiah said .\n",
            "`` And why did ye take away a man 's wife and take away his son ? '\n",
            "`` He was a man , I say . ' ''\n",
            "Isaiah said , `` I say , I am only a man from the land of Israel , ' because we come from the land of the heathen .\n",
            "We came from the heathen country , O Israel ; and we will give a ransom to our servants ' children to satisfy our demands .\n",
            "And that ransom they will give us , and make the children of Israel ' children and say , the sons of Israel shall be your sons , when they grow up and say to the sons of the land of Judah , the sons of the land of the heathen .\n",
            "These are your children unto the day of judgment ' ; so Isaiah spake .\n",
            "`` And then the people of Israel were not yet so far advanced in civilization as they were in their youth ; and they feared to slay their children , '' Isaiah said .\n",
            "`` But these wicked people came and slew my mother , and I took the daughter into the house of my father 's wife and brought her forth to my mother , not so young , but to a woman that was of the land of the heathen .\n",
            "Now I was too young to have sex with her , so she carried me on her bosom . ''\n",
            "`` Now I , too , was too young for sex with her , '' Isaiah continued .\n",
            "`` What I did , thou sayest .\n",
            "Now as it was this daughter of thy father 's wife - of the land of the heathen - was in the way of the righteous men 's righteousness .\n",
            "And so they called to her saying , saying , \" Why have ye put thy hand into the path of righteousness?\n",
            "Why have ye taken her to thy husband 's house and put her down there , O daughter of thy father 's wife , and not brought her to our house ?\n",
            "It is my people that he is the king of the heathen .\n",
            "Now do thou sayest to the children of Israel , ` Come to us , O daughters of thy father 's wife , and say to your mother , ` Tell your fathers to send me my child back here to thy father 's wife for her sake ; so I will send them my daughter . ' ''\n",
            "So Isaiah spake .\n",
            "`` And in the same way the people of Israel who were living in the land of Judah came and fought , and fought with their swords and guns , and their swords were not against them .\n",
            "And the people of the land of the heathen went to the house of their fathers - the sons of Jerusalem - and said to them , ` Go and\n",
            "\n",
            "[500 | 942.28] loss=3.20 avg=2.95\n",
            "[501 | 943.91] loss=2.63 avg=2.95\n",
            "[502 | 945.55] loss=3.19 avg=2.95\n",
            "[503 | 947.18] loss=3.23 avg=2.96\n",
            "[504 | 948.81] loss=2.53 avg=2.95\n",
            "[505 | 950.44] loss=2.48 avg=2.95\n",
            "[506 | 952.06] loss=2.79 avg=2.94\n",
            "[507 | 953.69] loss=2.34 avg=2.94\n",
            "[508 | 955.32] loss=3.44 avg=2.94\n",
            "[509 | 956.94] loss=2.64 avg=2.94\n",
            "[510 | 958.57] loss=3.00 avg=2.94\n",
            "[511 | 960.20] loss=3.69 avg=2.95\n",
            "[512 | 961.83] loss=3.17 avg=2.95\n",
            "[513 | 963.45] loss=2.17 avg=2.94\n",
            "[514 | 965.07] loss=3.34 avg=2.95\n",
            "[515 | 966.70] loss=2.40 avg=2.94\n",
            "[516 | 968.33] loss=2.84 avg=2.94\n",
            "[517 | 969.96] loss=2.26 avg=2.93\n",
            "[518 | 971.59] loss=2.30 avg=2.93\n",
            "[519 | 973.21] loss=3.26 avg=2.93\n",
            "[520 | 974.84] loss=2.75 avg=2.93\n",
            "[521 | 976.47] loss=2.69 avg=2.93\n",
            "[522 | 978.10] loss=2.66 avg=2.92\n",
            "[523 | 979.73] loss=3.11 avg=2.93\n",
            "[524 | 981.36] loss=2.56 avg=2.92\n",
            "[525 | 982.99] loss=2.98 avg=2.92\n",
            "[526 | 984.62] loss=3.40 avg=2.93\n",
            "[527 | 986.24] loss=2.45 avg=2.92\n",
            "[528 | 987.87] loss=3.30 avg=2.93\n",
            "[529 | 989.50] loss=2.98 avg=2.93\n",
            "[530 | 991.13] loss=2.85 avg=2.93\n",
            "[531 | 992.76] loss=2.83 avg=2.93\n",
            "[532 | 994.39] loss=3.01 avg=2.93\n",
            "[533 | 996.02] loss=2.91 avg=2.93\n",
            "[534 | 997.65] loss=2.74 avg=2.92\n",
            "[535 | 999.28] loss=2.39 avg=2.92\n",
            "[536 | 1000.89] loss=2.83 avg=2.92\n",
            "[537 | 1002.52] loss=3.10 avg=2.92\n",
            "[538 | 1004.15] loss=3.22 avg=2.92\n",
            "[539 | 1005.78] loss=3.46 avg=2.93\n",
            "[540 | 1007.41] loss=2.46 avg=2.92\n",
            "[541 | 1009.04] loss=3.30 avg=2.93\n",
            "[542 | 1010.67] loss=3.21 avg=2.93\n",
            "[543 | 1012.30] loss=2.39 avg=2.92\n",
            "[544 | 1013.92] loss=2.77 avg=2.92\n",
            "[545 | 1015.55] loss=2.94 avg=2.92\n",
            "[546 | 1017.18] loss=2.86 avg=2.92\n",
            "[547 | 1018.80] loss=3.41 avg=2.93\n",
            "[548 | 1020.43] loss=2.70 avg=2.93\n",
            "[549 | 1022.06] loss=3.19 avg=2.93\n",
            "[550 | 1023.70] loss=3.18 avg=2.93\n",
            "[551 | 1025.31] loss=2.61 avg=2.93\n",
            "[552 | 1026.94] loss=3.34 avg=2.93\n",
            "[553 | 1028.57] loss=3.26 avg=2.93\n",
            "[554 | 1030.20] loss=3.34 avg=2.94\n",
            "[555 | 1031.83] loss=3.25 avg=2.94\n",
            "[556 | 1033.46] loss=3.63 avg=2.95\n",
            "[557 | 1035.09] loss=2.87 avg=2.95\n",
            "[558 | 1036.72] loss=2.63 avg=2.94\n",
            "[559 | 1038.36] loss=2.50 avg=2.94\n",
            "[560 | 1039.99] loss=3.35 avg=2.94\n",
            "[561 | 1041.62] loss=3.30 avg=2.95\n",
            "[562 | 1043.26] loss=2.72 avg=2.95\n",
            "[563 | 1044.89] loss=2.42 avg=2.94\n",
            "[564 | 1046.52] loss=2.92 avg=2.94\n",
            "[565 | 1048.15] loss=2.52 avg=2.94\n",
            "[566 | 1049.78] loss=2.91 avg=2.94\n",
            "[567 | 1051.41] loss=2.28 avg=2.93\n",
            "[568 | 1053.04] loss=3.02 avg=2.93\n",
            "[569 | 1054.67] loss=2.73 avg=2.93\n",
            "[570 | 1056.30] loss=3.22 avg=2.93\n",
            "[571 | 1057.93] loss=2.83 avg=2.93\n",
            "[572 | 1059.56] loss=3.17 avg=2.93\n",
            "[573 | 1061.19] loss=2.57 avg=2.93\n",
            "[574 | 1062.82] loss=3.00 avg=2.93\n",
            "[575 | 1064.46] loss=2.56 avg=2.93\n",
            "[576 | 1066.09] loss=3.59 avg=2.93\n",
            "[577 | 1067.71] loss=3.21 avg=2.94\n",
            "[578 | 1069.34] loss=2.36 avg=2.93\n",
            "[579 | 1070.97] loss=2.60 avg=2.93\n",
            "[580 | 1072.60] loss=2.88 avg=2.93\n",
            "[581 | 1074.22] loss=2.63 avg=2.92\n",
            "[582 | 1075.86] loss=2.66 avg=2.92\n",
            "[583 | 1077.48] loss=2.50 avg=2.92\n",
            "[584 | 1079.12] loss=2.95 avg=2.92\n",
            "[585 | 1080.75] loss=3.40 avg=2.92\n",
            "[586 | 1082.38] loss=3.19 avg=2.92\n",
            "[587 | 1084.01] loss=2.81 avg=2.92\n",
            "[588 | 1085.64] loss=2.61 avg=2.92\n",
            "[589 | 1087.27] loss=2.98 avg=2.92\n",
            "[590 | 1088.90] loss=3.74 avg=2.93\n",
            "[591 | 1090.54] loss=3.05 avg=2.93\n",
            "[592 | 1092.16] loss=3.24 avg=2.93\n",
            "[593 | 1093.79] loss=3.09 avg=2.93\n",
            "[594 | 1095.43] loss=3.61 avg=2.94\n",
            "[595 | 1097.06] loss=2.86 avg=2.94\n",
            "[596 | 1098.69] loss=3.20 avg=2.94\n",
            "[597 | 1100.31] loss=2.79 avg=2.94\n",
            "[598 | 1101.94] loss=2.70 avg=2.94\n",
            "[599 | 1103.57] loss=2.90 avg=2.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " when some old dandy was sitting there , saying that he thought the old men well behaved , and was quite a good man , he could never forgive them , so he had taken a sword out of his pocket and used it to cut their necks .\n",
            "When they started out to make their excuses , and were beginning to ask the poor fellow , '' I explained , `` I will show you to be not so mean and cowardly after all . ''\n",
            "`` Oh ! ''\n",
            "said my father , going down the stairs , `` you are a fool ! ''\n",
            "`` How ! ''\n",
            "said his wife anxiously , `` what , for no other reason is there but to hurt people , '' and they went to get me off their necks .\n",
            "The next morning , the king called upon him and informed him of what I had done , and asked that he might have something to say in his behalf , whether it were a good thing or not , which he said he would .\n",
            "Then the young man told him to go to sleep , for he was going out to hunt for something to eat , but the king could see all that he had done , and was so angry that the youngest man was left behind , and the old man had only to tell him that he was never to get on with the new king , or else be made rich .\n",
            "He said to this young man : `` How come my father 's sword is made of a hard stone ? ''\n",
            "`` It 's got that hard stone too , '' replied the young man , with a face full of tears , and for three days and three nights he sat awake day and night , and the king listened to him as his wife talked to him .\n",
            "The king went out and hunted for some fish , and soon there came the king 's men to collect what he might have taken .\n",
            "The young man went to the well , but when they tried to catch it their hands slipped under the surface , for they were all full of flies and bugs .\n",
            "They had scarcely got him to the water when several old men saw them , and when they started to attack them they gave one another a hard shove , and then both fell unconscious .\n",
            "At this , the young man went to the king 's palace , and told him what he had done , and he asked to speak to the king .\n",
            "`` My father has been so cruel to you , '' said this young king , `` how do you blame him ? ''\n",
            "`` Well , '' he answered , `` I have the courage of my age , and I say that he deserves what he has given you . ''\n",
            "`` He hath proved you right , '' said I , `` you did right in his kindness , '' and I promised that if he could only help me out of prison , I would give the king his sword , for the king had said that it was a wicked thing to have it , so he would take it , and I never knew why I told him that , but the king was so angry at this , and told me to tell him to be on his guard , and never ever to get it , that when he should get hold of it and try to use it , he would die .\n",
            "`` Oh ! ''\n",
            "said he at last , `` you 're too hard on yourself , '' and he left off , for there we had no other people on whom he could tell his secrets .\n",
            "I never knew how much he loved and cared for Mein Kampf , till it was time to begin again , for the young king had died suddenly the whole winter , and so he was no longer fit for any part of it , as I am now .\n",
            "But if he could not get the sword away I could not , and all the time I was sitting on that tree , and the sun was shining in through the gaps .\n",
            "The whole world watched and awaited my progress in the wilderness , and I knew that my father 's sword had been carried away too , and the old man was so angry at me that he could not take it , and my husband had only to tell me that if this young man , now that he was no longer able to bear the young king 's threats , would only say what he could to help him , he would get it .\n",
            "When this message came , I sat up and went to the old man 's house , and gave him my sword , and went away , crying softly , ' How will you do ? '\n",
            "`` Oh ! ''\n",
            "his wife thought , and began to follow , because she saw me walk home , and the only thing she could do to help me would be to go with me to the king 's palace and tell him that he must have my sword .\n",
            "I thought no less than two men might go up in the moon to ask for it , but I never ever went to the palace as I do now , unless it was to make an offer of my life , for it is always very hot up there , and in the moonlight it is much better to be safe\n",
            "\n",
            "[600 | 1129.04] loss=2.44 avg=2.93\n",
            "[601 | 1130.67] loss=2.98 avg=2.93\n",
            "[602 | 1132.31] loss=2.98 avg=2.93\n",
            "[603 | 1133.95] loss=3.10 avg=2.94\n",
            "[604 | 1135.58] loss=2.76 avg=2.93\n",
            "[605 | 1137.22] loss=2.75 avg=2.93\n",
            "[606 | 1138.85] loss=2.46 avg=2.93\n",
            "[607 | 1140.48] loss=2.82 avg=2.93\n",
            "[608 | 1142.11] loss=2.16 avg=2.92\n",
            "[609 | 1143.74] loss=2.97 avg=2.92\n",
            "[610 | 1145.38] loss=2.58 avg=2.92\n",
            "[611 | 1147.01] loss=2.75 avg=2.91\n",
            "[612 | 1148.64] loss=2.28 avg=2.91\n",
            "[613 | 1150.27] loss=3.36 avg=2.91\n",
            "[614 | 1151.90] loss=2.79 avg=2.91\n",
            "[615 | 1153.53] loss=3.04 avg=2.91\n",
            "[616 | 1155.16] loss=2.71 avg=2.91\n",
            "[617 | 1156.79] loss=3.04 avg=2.91\n",
            "[618 | 1158.43] loss=2.30 avg=2.91\n",
            "[619 | 1160.06] loss=3.15 avg=2.91\n",
            "[620 | 1161.69] loss=3.10 avg=2.91\n",
            "[621 | 1163.31] loss=2.50 avg=2.91\n",
            "[622 | 1164.94] loss=2.90 avg=2.91\n",
            "[623 | 1166.57] loss=3.27 avg=2.91\n",
            "[624 | 1168.20] loss=3.61 avg=2.92\n",
            "[625 | 1169.83] loss=2.78 avg=2.92\n",
            "[626 | 1171.47] loss=2.48 avg=2.91\n",
            "[627 | 1173.10] loss=3.33 avg=2.92\n",
            "[628 | 1174.73] loss=2.73 avg=2.91\n",
            "[629 | 1176.36] loss=2.99 avg=2.91\n",
            "[630 | 1177.99] loss=3.00 avg=2.91\n",
            "[631 | 1179.62] loss=3.28 avg=2.92\n",
            "[632 | 1181.25] loss=3.21 avg=2.92\n",
            "[633 | 1182.89] loss=2.93 avg=2.92\n",
            "[634 | 1184.52] loss=3.27 avg=2.93\n",
            "[635 | 1186.15] loss=3.09 avg=2.93\n",
            "[636 | 1187.78] loss=3.08 avg=2.93\n",
            "[637 | 1189.41] loss=3.21 avg=2.93\n",
            "[638 | 1191.04] loss=2.49 avg=2.93\n",
            "[639 | 1192.67] loss=2.68 avg=2.92\n",
            "[640 | 1194.30] loss=3.04 avg=2.93\n",
            "[641 | 1195.93] loss=3.04 avg=2.93\n",
            "[642 | 1197.56] loss=2.20 avg=2.92\n",
            "[643 | 1199.19] loss=3.05 avg=2.92\n",
            "[644 | 1200.82] loss=3.07 avg=2.92\n",
            "[645 | 1202.46] loss=2.66 avg=2.92\n",
            "[646 | 1204.09] loss=2.39 avg=2.91\n",
            "[647 | 1205.72] loss=3.50 avg=2.92\n",
            "[648 | 1207.35] loss=3.22 avg=2.92\n",
            "[649 | 1208.99] loss=3.14 avg=2.93\n",
            "[650 | 1210.60] loss=3.18 avg=2.93\n",
            "[651 | 1212.23] loss=2.40 avg=2.92\n",
            "[652 | 1213.87] loss=2.65 avg=2.92\n",
            "[653 | 1215.50] loss=2.95 avg=2.92\n",
            "[654 | 1217.14] loss=2.48 avg=2.92\n",
            "[655 | 1218.77] loss=3.12 avg=2.92\n",
            "[656 | 1220.40] loss=3.15 avg=2.92\n",
            "[657 | 1222.03] loss=2.49 avg=2.92\n",
            "[658 | 1223.66] loss=2.78 avg=2.91\n",
            "[659 | 1225.30] loss=3.28 avg=2.92\n",
            "[660 | 1226.93] loss=2.69 avg=2.92\n",
            "[661 | 1228.56] loss=3.02 avg=2.92\n",
            "[662 | 1230.19] loss=3.34 avg=2.92\n",
            "[663 | 1231.83] loss=2.80 avg=2.92\n",
            "[664 | 1233.46] loss=2.47 avg=2.92\n",
            "[665 | 1235.09] loss=3.07 avg=2.92\n",
            "[666 | 1236.72] loss=2.47 avg=2.91\n",
            "[667 | 1238.35] loss=3.07 avg=2.91\n",
            "[668 | 1239.98] loss=3.02 avg=2.91\n",
            "[669 | 1241.61] loss=2.88 avg=2.91\n",
            "[670 | 1243.23] loss=2.41 avg=2.91\n",
            "[671 | 1244.86] loss=3.19 avg=2.91\n",
            "[672 | 1246.49] loss=3.79 avg=2.92\n",
            "[673 | 1248.13] loss=2.76 avg=2.92\n",
            "[674 | 1249.76] loss=2.98 avg=2.92\n",
            "[675 | 1251.40] loss=2.53 avg=2.92\n",
            "[676 | 1253.03] loss=2.37 avg=2.91\n",
            "[677 | 1254.66] loss=2.46 avg=2.91\n",
            "[678 | 1256.30] loss=3.28 avg=2.91\n",
            "[679 | 1257.93] loss=3.42 avg=2.91\n",
            "[680 | 1259.57] loss=3.31 avg=2.92\n",
            "[681 | 1261.19] loss=3.17 avg=2.92\n",
            "[682 | 1262.82] loss=3.29 avg=2.92\n",
            "[683 | 1264.45] loss=2.98 avg=2.93\n",
            "[684 | 1266.08] loss=2.75 avg=2.92\n",
            "[685 | 1267.71] loss=2.96 avg=2.92\n",
            "[686 | 1269.34] loss=2.64 avg=2.92\n",
            "[687 | 1270.97] loss=3.07 avg=2.92\n",
            "[688 | 1272.61] loss=2.41 avg=2.92\n",
            "[689 | 1274.24] loss=3.29 avg=2.92\n",
            "[690 | 1275.87] loss=2.95 avg=2.92\n",
            "[691 | 1277.50] loss=3.37 avg=2.93\n",
            "[692 | 1279.13] loss=2.96 avg=2.93\n",
            "[693 | 1280.76] loss=2.57 avg=2.92\n",
            "[694 | 1282.40] loss=2.62 avg=2.92\n",
            "[695 | 1284.03] loss=2.60 avg=2.92\n",
            "[696 | 1285.66] loss=3.18 avg=2.92\n",
            "[697 | 1287.29] loss=3.09 avg=2.92\n",
            "[698 | 1288.92] loss=3.14 avg=2.92\n",
            "[699 | 1290.56] loss=3.09 avg=2.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " uncomfortable and had a pretty bad winter .\n",
            "When I arrived in town last January I found there were two new-comers : the very clever Mrs. James and the very clever Miss Susan . Both were already at St. Mungo's and were beginning to get used to it , as soon as they began to speak .\n",
            "Miss Susan was a pretty girl with a nice brown complexion and a pretty face , and she looked very pretty , as long as she could talk .\n",
            "She had a little bag of books about her , and they were very good , because they were mostly about medicine and healing , but she had quite a fair time talking to Miss James about her books , which were about everything .\n",
            "She made a pretty little hole in one of the books and wrote a little story in it about it , and said she liked to read the kind of books she was writing most of the time , while Miss James kept the rest of the books , and he told her he would write a story for her if she was willing to teach him .\n",
            "I had no business teaching her , but I did like her very much , and I always felt sorry when I thought myself to have been kind to her , and she always looked so much like some dear old grandma , I never dared to ask her if she did ever like me .\n",
            "She had started from a very poor background , and could not even work on Monday and Tuesday , while all the time her parents and grandparents were always busy with work and home ; and she was very sorry , and very ashamed of herself for not being able to help her grandparents to work a little in their little green house and give them some bread and milk every day .\n",
            "Mrs. James was a rich girl , and when she was given her chance in life she did very well indeed .\n",
            "Her father died a few months before I came along , and her mother took her away from her ailing mother and set her up in a boarding-school .\n",
            "I found her there and was glad to see her so happy and well-fed , not because of her own fault .\n",
            "As we started off we talked in a very serious tone , and then Miss Susan started , and I had no time to speak .\n",
            "I was so surprised that I took a good deal of trouble to make myself pleasant and not to make a nuisance of myself .\n",
            "What a difference , my dear -- there is always a difference !\n",
            "She said so much that at last I did give in , and tried to do my best , but I still did not manage .\n",
            "She was so good at picking up that thing called humour , and not only did I do her a sort of kindness , but I found my own humour and it made her laugh and make me blush .\n",
            "She laughed so often and with both her hands at her breast .\n",
            "I could not help it -- she liked it , and was very good , very funny , very very nice , I could tell .\n",
            "When she started to tell her stories again I was a little uneasy .\n",
            "In my eyes , she looked so much like the older sister , who was the prettier girl and was better-looking -- the sort of girl I used to love to see in the play .\n",
            "It struck me as strange that I should like a sister now , and she made more sense to me than I had ever thought .\n",
            "She told me all about her childhood , and how she had once been very poor , and had only the money that she could throw at her play , and how she had gone to live with her grandma and lived very happily , always looking after the children .\n",
            "Of all people else she ought to have loved me , and she did .\n",
            "But I felt that she was not very good at talking , and I had to ask her , \" what was it you want now ? '\n",
            "I was afraid that her way of living was going to suffer , but the children kept reminding her that her mother was so poor she could not help her , and that in her absence she should learn to work instead .\n",
            "She agreed to do that , but only if she could help to work , and she soon proved that she could .\n",
            "In three months she did -- but that was to the great disappointment of Miss Susan .\n",
            "After four months she had finished the play she had started , and in two days she returned home with a fresh start .\n",
            "She did not think she knew how to work , and had no idea how to cook , or what to look down she got every day for four quarters of a day , as Mrs. James did , for two hours a day , and it was quite exhausting .\n",
            "Of course she did not like to be in bed all the time , and at first she did well , when Mrs. James was away for a week , but she gradually became very cold and disheartened , and Mrs. James always looked for excuses to get her into bed .\n",
            "The next month was very dismal for Miss Susan .\n",
            "Her mother was dying , and the other days she was\n",
            "\n",
            "[700 | 1315.77] loss=3.00 avg=2.93\n",
            "[701 | 1317.40] loss=3.77 avg=2.93\n",
            "[702 | 1319.04] loss=2.42 avg=2.93\n",
            "[703 | 1320.67] loss=2.82 avg=2.93\n",
            "[704 | 1322.30] loss=3.23 avg=2.93\n",
            "[705 | 1323.94] loss=2.43 avg=2.93\n",
            "[706 | 1325.57] loss=2.78 avg=2.92\n",
            "[707 | 1327.21] loss=2.89 avg=2.92\n",
            "[708 | 1328.84] loss=3.02 avg=2.92\n",
            "[709 | 1330.47] loss=3.07 avg=2.93\n",
            "[710 | 1332.10] loss=3.65 avg=2.93\n",
            "[711 | 1333.73] loss=2.82 avg=2.93\n",
            "[712 | 1335.36] loss=2.54 avg=2.93\n",
            "[713 | 1336.99] loss=2.68 avg=2.93\n",
            "[714 | 1338.63] loss=3.30 avg=2.93\n",
            "[715 | 1340.26] loss=2.49 avg=2.93\n",
            "[716 | 1341.88] loss=3.27 avg=2.93\n",
            "[717 | 1343.51] loss=3.22 avg=2.93\n",
            "[718 | 1345.14] loss=2.79 avg=2.93\n",
            "[719 | 1346.78] loss=2.99 avg=2.93\n",
            "[720 | 1348.41] loss=2.85 avg=2.93\n",
            "[721 | 1350.04] loss=3.24 avg=2.93\n",
            "[722 | 1351.67] loss=2.46 avg=2.93\n",
            "[723 | 1353.30] loss=2.52 avg=2.92\n",
            "[724 | 1354.93] loss=2.70 avg=2.92\n",
            "[725 | 1356.55] loss=3.09 avg=2.92\n",
            "[726 | 1358.18] loss=2.53 avg=2.92\n",
            "[727 | 1359.81] loss=2.77 avg=2.92\n",
            "[728 | 1361.44] loss=2.46 avg=2.91\n",
            "[729 | 1363.07] loss=2.39 avg=2.91\n",
            "[730 | 1364.70] loss=2.39 avg=2.90\n",
            "[731 | 1366.33] loss=2.90 avg=2.90\n",
            "[732 | 1367.97] loss=2.40 avg=2.90\n",
            "[733 | 1369.60] loss=2.72 avg=2.90\n",
            "[734 | 1371.23] loss=3.16 avg=2.90\n",
            "[735 | 1372.87] loss=3.16 avg=2.90\n",
            "[736 | 1374.50] loss=2.64 avg=2.90\n",
            "[737 | 1376.13] loss=2.31 avg=2.89\n",
            "[738 | 1377.76] loss=2.92 avg=2.89\n",
            "[739 | 1379.39] loss=3.25 avg=2.90\n",
            "[740 | 1381.02] loss=2.68 avg=2.89\n",
            "[741 | 1382.66] loss=2.69 avg=2.89\n",
            "[742 | 1384.29] loss=3.00 avg=2.89\n",
            "[743 | 1385.92] loss=2.27 avg=2.89\n",
            "[744 | 1387.55] loss=2.93 avg=2.89\n",
            "[745 | 1389.19] loss=2.84 avg=2.89\n",
            "[746 | 1390.82] loss=3.19 avg=2.89\n",
            "[747 | 1392.45] loss=3.25 avg=2.89\n",
            "[748 | 1394.09] loss=2.90 avg=2.89\n",
            "[749 | 1395.72] loss=3.16 avg=2.90\n",
            "[750 | 1397.36] loss=2.16 avg=2.89\n",
            "[751 | 1398.98] loss=3.17 avg=2.89\n",
            "[752 | 1400.61] loss=2.86 avg=2.89\n",
            "[753 | 1402.23] loss=3.46 avg=2.90\n",
            "[754 | 1403.86] loss=2.73 avg=2.90\n",
            "[755 | 1405.49] loss=3.21 avg=2.90\n",
            "[756 | 1407.12] loss=2.78 avg=2.90\n",
            "[757 | 1408.75] loss=2.61 avg=2.90\n",
            "[758 | 1410.38] loss=3.30 avg=2.90\n",
            "[759 | 1412.02] loss=3.45 avg=2.90\n",
            "[760 | 1413.65] loss=3.14 avg=2.91\n",
            "[761 | 1415.29] loss=3.09 avg=2.91\n",
            "[762 | 1416.92] loss=2.43 avg=2.90\n",
            "[763 | 1418.55] loss=2.91 avg=2.90\n",
            "[764 | 1420.18] loss=2.29 avg=2.90\n",
            "[765 | 1421.81] loss=2.53 avg=2.89\n",
            "[766 | 1423.44] loss=3.25 avg=2.90\n",
            "[767 | 1425.06] loss=3.36 avg=2.90\n",
            "[768 | 1426.70] loss=2.77 avg=2.90\n",
            "[769 | 1428.32] loss=3.41 avg=2.91\n",
            "[770 | 1429.96] loss=3.07 avg=2.91\n",
            "[771 | 1431.59] loss=4.00 avg=2.92\n",
            "[772 | 1433.22] loss=3.05 avg=2.92\n",
            "[773 | 1434.86] loss=2.33 avg=2.91\n",
            "[774 | 1436.49] loss=3.38 avg=2.92\n",
            "[775 | 1438.12] loss=3.03 avg=2.92\n",
            "[776 | 1439.75] loss=3.62 avg=2.93\n",
            "[777 | 1441.39] loss=2.80 avg=2.93\n",
            "[778 | 1443.02] loss=2.90 avg=2.93\n",
            "[779 | 1444.65] loss=2.96 avg=2.93\n",
            "[780 | 1446.28] loss=3.40 avg=2.93\n",
            "[781 | 1447.91] loss=2.83 avg=2.93\n",
            "[782 | 1449.54] loss=3.17 avg=2.93\n",
            "[783 | 1451.17] loss=3.16 avg=2.93\n",
            "[784 | 1452.80] loss=2.61 avg=2.93\n",
            "[785 | 1454.44] loss=3.11 avg=2.93\n",
            "[786 | 1456.07] loss=2.93 avg=2.93\n",
            "[787 | 1457.70] loss=2.83 avg=2.93\n",
            "[788 | 1459.34] loss=3.41 avg=2.94\n",
            "[789 | 1460.97] loss=2.77 avg=2.94\n",
            "[790 | 1462.60] loss=3.35 avg=2.94\n",
            "[791 | 1464.23] loss=3.14 avg=2.94\n",
            "[792 | 1465.86] loss=2.99 avg=2.94\n",
            "[793 | 1467.49] loss=2.61 avg=2.94\n",
            "[794 | 1469.12] loss=3.22 avg=2.94\n",
            "[795 | 1470.76] loss=2.65 avg=2.94\n",
            "[796 | 1472.39] loss=3.29 avg=2.94\n",
            "[797 | 1474.02] loss=3.24 avg=2.94\n",
            "[798 | 1475.65] loss=3.17 avg=2.95\n",
            "[799 | 1477.29] loss=3.00 avg=2.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " wish to tell you about it .\n",
            "I 'm terribly ill , and I can not keep my mouth shut .\n",
            "I beg of you , if you will take a look at my poor little heart , will you tell me what it 's like ? '\n",
            "` Only if you will promise me to marry that young lady .\n",
            "You have seen her in a carriage and the one she has been with .\n",
            "Do n't be angry at her , but look at what she does with her life ; she is a fine bride , but she does not love her as much as she loves others .\n",
            "They say she can not get a second husband , that is not true , but I 'm sure she has more pleasure in one .\n",
            "She loves her little heart to the last , but is not the sort of girl who ever wants a husband .\n",
            "So far as I know there never was a man in the world whom her heart took the greatest pleasure in .\n",
            "Of course , it was not the kindest of men in the world , but it was not cruel that I have never had a man to love me .\n",
            "I am always ashamed of myself for not having loved her .\n",
            "I was much surprised when I heard she had died . '\n",
            "` A woman died for me one day .\n",
            "It was a pleasant day in the spring of last year , and the sun was almost out .\n",
            "As you know , my dear , she was a man 's wife and I took her to be my bride ; so it made me feel quite happy to have her to love me .\n",
            "When I went downstairs in the late summer she was gone -- no one knows how she left -- and then I went to my room and there I found her in that strange carriage , dressed as if for a wedding , which she never had been and will never be . '\n",
            "` It was a pretty night in May ; the girls were dancing with the boys at the wedding dinner and it made me very happy .\n",
            "When I heard she had died I did n't believe that it was all my fault , but I was very sad .\n",
            "I really thought I had done her a hurt .\n",
            "I would have had such fun if I had married that woman myself .\n",
            "I do n't remember the name of that poor maid ; perhaps I do not seem to be her brother . '\n",
            "` Well , if she had not been the kindest of men I should have loved her , but I did n't mean to get too fond of her .\n",
            "Well , it had to be done I suppose , she was very sick , and she only went out to dance with other girls to make up for her absence .\n",
            "I heard that she had not lived very long , but it was a long time ago . '\n",
            "` You have not known her much , my lovely , but I have seen her a quarter of a dozen times as she walks from the house of the maid to where her room was in the great hall and where we will dance to-night .\n",
            "When she is not there I like to have a good look at her .\n",
            "It is a sight to live through .\n",
            "It makes you a more loving person .\n",
            "She is so full of life .\n",
            "I see all the time why we are all so unhappy . '\n",
            "` But you are her keeper , and I wish you good fortune .\n",
            "I am very old and am quite tired sometimes .\n",
            "I do n't think I 'll need a lot of good luck now .\n",
            "We are in the midst of a terrible war and I do n't think that can end any time . '\n",
            "` It is no use worrying about it , my dear .\n",
            "You have made me a whole lot happier and wiser .\n",
            "We shall have good luck to keep in mind before it is too late . '\n",
            "` Well , that is good luck and you are so good to me .\n",
            "Oh , you are a pretty dear , but I have not had much fortune before .\n",
            "You are the first dear I ever loved : I hope I will have another one .\n",
            "It makes you glad . '\n",
            "` It does n't do me any harm to say this , but this girl 's name is Mary . `\n",
            "` Oh , I 'm afraid you have not been near anyone 's mother or father .\n",
            "She is very old and has no idea that her children do n't love her as much as she does .\n",
            "I like to sit and talk to them for naught .\n",
            "I would n't be so silly as to try to talk my way into her heart , but I do n't have one . '\n",
            "` Well , you will know if you come at one o'clock this afternoon and find the door locked .\n",
            "You only need take a look at her face , which looks very very happy from a little distance , with her beautiful big brown eyes and her lovely little black eyebrows .\n",
            "Well , I am glad you are at all well , and I love you very much now . '\n",
            "\n",
            "\n",
            "[800 | 1501.91] loss=3.19 avg=2.95\n",
            "[801 | 1503.55] loss=2.60 avg=2.95\n",
            "[802 | 1505.18] loss=3.13 avg=2.95\n",
            "[803 | 1506.82] loss=2.27 avg=2.94\n",
            "[804 | 1508.45] loss=2.14 avg=2.93\n",
            "[805 | 1510.08] loss=3.46 avg=2.94\n",
            "[806 | 1511.71] loss=2.26 avg=2.93\n",
            "[807 | 1513.35] loss=2.96 avg=2.93\n",
            "[808 | 1514.99] loss=3.03 avg=2.93\n",
            "[809 | 1516.63] loss=3.85 avg=2.94\n",
            "[810 | 1518.26] loss=2.52 avg=2.94\n",
            "[811 | 1519.89] loss=2.81 avg=2.94\n",
            "[812 | 1521.53] loss=3.36 avg=2.94\n",
            "[813 | 1523.17] loss=2.46 avg=2.94\n",
            "[814 | 1524.80] loss=2.86 avg=2.94\n",
            "[815 | 1526.42] loss=2.93 avg=2.94\n",
            "[816 | 1528.05] loss=2.38 avg=2.93\n",
            "[817 | 1529.68] loss=3.49 avg=2.94\n",
            "[818 | 1531.32] loss=2.92 avg=2.94\n",
            "[819 | 1532.96] loss=2.49 avg=2.93\n",
            "[820 | 1534.59] loss=3.27 avg=2.93\n",
            "[821 | 1536.22] loss=3.02 avg=2.94\n",
            "[822 | 1537.85] loss=3.44 avg=2.94\n",
            "[823 | 1539.48] loss=3.25 avg=2.94\n",
            "[824 | 1541.11] loss=3.34 avg=2.95\n",
            "[825 | 1542.73] loss=2.94 avg=2.95\n",
            "[826 | 1544.36] loss=3.47 avg=2.95\n",
            "[827 | 1545.98] loss=2.50 avg=2.95\n",
            "[828 | 1547.61] loss=2.64 avg=2.94\n",
            "[829 | 1549.24] loss=2.99 avg=2.95\n",
            "[830 | 1550.87] loss=2.57 avg=2.94\n",
            "[831 | 1552.48] loss=2.58 avg=2.94\n",
            "[832 | 1554.11] loss=3.39 avg=2.94\n",
            "[833 | 1555.74] loss=3.60 avg=2.95\n",
            "[834 | 1557.36] loss=3.17 avg=2.95\n",
            "[835 | 1558.99] loss=3.34 avg=2.96\n",
            "[836 | 1560.62] loss=3.16 avg=2.96\n",
            "[837 | 1562.25] loss=2.70 avg=2.95\n",
            "[838 | 1563.88] loss=2.52 avg=2.95\n",
            "[839 | 1565.50] loss=3.04 avg=2.95\n",
            "[840 | 1567.12] loss=3.60 avg=2.96\n",
            "[841 | 1568.74] loss=2.26 avg=2.95\n",
            "[842 | 1570.36] loss=2.28 avg=2.94\n",
            "[843 | 1571.99] loss=3.61 avg=2.95\n",
            "[844 | 1573.62] loss=2.07 avg=2.94\n",
            "[845 | 1575.25] loss=2.12 avg=2.93\n",
            "[846 | 1576.88] loss=2.98 avg=2.93\n",
            "[847 | 1578.51] loss=3.12 avg=2.94\n",
            "[848 | 1580.14] loss=2.97 avg=2.94\n",
            "[849 | 1581.78] loss=2.44 avg=2.93\n",
            "[850 | 1583.40] loss=3.09 avg=2.93\n",
            "[851 | 1585.03] loss=2.49 avg=2.93\n",
            "[852 | 1586.66] loss=3.31 avg=2.93\n",
            "[853 | 1588.30] loss=2.93 avg=2.93\n",
            "[854 | 1589.93] loss=3.14 avg=2.93\n",
            "[855 | 1591.57] loss=2.58 avg=2.93\n",
            "[856 | 1593.20] loss=2.66 avg=2.93\n",
            "[857 | 1594.84] loss=2.38 avg=2.92\n",
            "[858 | 1596.47] loss=2.50 avg=2.92\n",
            "[859 | 1598.11] loss=2.92 avg=2.92\n",
            "[860 | 1599.74] loss=3.17 avg=2.92\n",
            "[861 | 1601.38] loss=3.19 avg=2.92\n",
            "[862 | 1603.01] loss=2.69 avg=2.92\n",
            "[863 | 1604.65] loss=2.55 avg=2.92\n",
            "[864 | 1606.27] loss=2.91 avg=2.92\n",
            "[865 | 1607.91] loss=2.32 avg=2.91\n",
            "[866 | 1609.55] loss=2.41 avg=2.91\n",
            "[867 | 1611.18] loss=2.57 avg=2.90\n",
            "[868 | 1612.81] loss=3.18 avg=2.91\n",
            "[869 | 1614.45] loss=3.43 avg=2.91\n",
            "[870 | 1616.08] loss=3.01 avg=2.91\n",
            "[871 | 1617.72] loss=3.06 avg=2.91\n",
            "[872 | 1619.36] loss=2.42 avg=2.91\n",
            "[873 | 1620.99] loss=2.70 avg=2.91\n",
            "[874 | 1622.62] loss=3.60 avg=2.91\n",
            "[875 | 1624.25] loss=2.58 avg=2.91\n",
            "[876 | 1625.88] loss=3.37 avg=2.91\n",
            "[877 | 1627.51] loss=3.43 avg=2.92\n",
            "[878 | 1629.15] loss=2.89 avg=2.92\n",
            "[879 | 1630.78] loss=2.68 avg=2.92\n",
            "[880 | 1632.41] loss=3.13 avg=2.92\n",
            "[881 | 1634.05] loss=2.98 avg=2.92\n",
            "[882 | 1635.68] loss=3.04 avg=2.92\n",
            "[883 | 1637.31] loss=3.50 avg=2.93\n",
            "[884 | 1638.94] loss=3.42 avg=2.93\n",
            "[885 | 1640.57] loss=2.42 avg=2.93\n",
            "[886 | 1642.19] loss=3.40 avg=2.93\n",
            "[887 | 1643.82] loss=3.05 avg=2.93\n",
            "[888 | 1645.45] loss=2.25 avg=2.93\n",
            "[889 | 1647.09] loss=2.35 avg=2.92\n",
            "[890 | 1648.72] loss=2.54 avg=2.92\n",
            "[891 | 1650.35] loss=2.95 avg=2.92\n",
            "[892 | 1651.98] loss=2.84 avg=2.92\n",
            "[893 | 1653.61] loss=2.48 avg=2.91\n",
            "[894 | 1655.23] loss=2.33 avg=2.91\n",
            "[895 | 1656.86] loss=3.21 avg=2.91\n",
            "[896 | 1658.49] loss=2.81 avg=2.91\n",
            "[897 | 1660.12] loss=3.35 avg=2.91\n",
            "[898 | 1661.75] loss=2.83 avg=2.91\n",
            "[899 | 1663.38] loss=2.65 avg=2.91\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " get her out .\n",
            "I 'd never have done that .\n",
            "But I was hoping you would give her the money for her milk , and I 'd never have thought she would get it if I just let her . ' '\n",
            "Oh , my dear , I was , too .\n",
            "I could never have thought this 'd ever have happened . '\n",
            "` Well , now what 's my business ? '\n",
            "said Mr. Gladstone , as gently as a feather .\n",
            "` Well , it 's you 's business .\n",
            "You can see how you would never feel any need of paying for it yourself .\n",
            "But if it was just me , I thought about it .\n",
            "It was a very big affair , too , and it was a very happy thing indeed to see how well we all looked at each other after that first happy afternoon , and how we all thought it a great success . '\n",
            "` You would have laughed to see me smile then ? '\n",
            "he cried in a low , half-muttering voice .\n",
            "` There would have been a lot to laugh about , and that would have been such a shame , ' said Mrs. Gladstone .\n",
            "` You can have it , Mrs. Gladstone .\n",
            "It would certainly get you some good papers . '\n",
            "` Yes , I 'd have to have it , and I want it , now . ' '\n",
            "I 'm sorry I 'm sorry .\n",
            "I 'm very sorry I got carried away . ' '\n",
            "Mr. Gladstone , you 'll let her have the milk , and we 'll buy it up for her and see we can afford it , and that it would be more than worth it . '\n",
            "` No , I 'll never let her have the milk .\n",
            "I would never do that , and I promise I would never do that .\n",
            "She wants it now , but you can have it and you would have laughed if I had just told her no , though I promised I would never .\n",
            "Do not worry .\n",
            "I have nothing to hide from you , no matter what .\n",
            "If you tell the truth , it will be all right . '<|endoftext|>Cannabis extracts of different types have been widely studied for some years. In an attempt to increase their pharmacological activity, numerous studies have been carried out using some of them . However , none of them gave any results which would allow us to say with confidence whether their use should be taken as a new and useful medicine or as a means of causing a considerable decrease in cancer risk .\n",
            "Cannabis extracts of higher activity , which are not directly absorbed into the blood stream , have been employed in experiments where the effects were produced by means of a vapor inhalation , or where the inhalation had been of a very high character as a preventive measure .\n",
            "These extracts have been taken mainly at times of very serious depression such as in cases of alcoholism , insanity , or convulsions .\n",
            "The main result obtained in these cases was the reduction of the physical agony caused by physical exhaustion , and the restoration to a degree of normalcy and health which it formerly took months to regain .\n",
            "The principal effect which these extracts produced was a progressive weakening and cessation of the nervous energy which is the chief factor in the normalisation of emotional and mental states , which includes those of fatigue , nervous excitement and depression of temper .\n",
            "This is the primary effect which has been observed by various authors , and is the most satisfactory and most direct .\n",
            "By smoking , in which case there is no danger of the burning of the lungs when a patient , with the exception of a few individuals , the burning is usually due to the effects of the inhaling of a burning-powder .\n",
            "In general terms , however , the primary effect of inhalation and the effect of smoking have remained the same.\n",
            "This article will describe the pharmacological effects of the various extracts , but in describing the effects on the brain at some stage of their preparation , it has seemed important to take note of some important points , since their efficacy usually depends on their being given orally as a single dose , and without the necessity both of making the dose suitable for human consumption , and that the dose be administered in proportion to the patient 's state of nervous excitement , and the usual physiological influence on which it is due .\n",
            "A brief account of the different modes of preparation will be found at the end of the chapter .\n",
            "The Effects of Tobacco Smoking\n",
            "Tobacco smoke , being an oxidizing agent in the process of formation of the tar of the plant , presents a great danger to the health and the morals of the user .\n",
            "If not handled carelessly , even with a careful nose or with the use of some kind of decoction , and if not taken with food or water , and should be given during periods of severe sleep , such as are usually associated with intoxication of the patient , the dangerous effects may result , and even die , without prompt treatment .\n",
            "If the user has drunk his share of wine or beer , or has taken tobacco\n",
            "\n",
            "[900 | 1688.35] loss=2.24 avg=2.90\n",
            "[901 | 1689.98] loss=3.10 avg=2.90\n",
            "[902 | 1691.61] loss=2.25 avg=2.90\n",
            "[903 | 1693.24] loss=2.95 avg=2.90\n",
            "[904 | 1694.87] loss=2.79 avg=2.90\n",
            "[905 | 1696.50] loss=2.16 avg=2.89\n",
            "[906 | 1698.13] loss=2.94 avg=2.89\n",
            "[907 | 1699.76] loss=2.51 avg=2.89\n",
            "[908 | 1701.38] loss=2.30 avg=2.88\n",
            "[909 | 1703.01] loss=3.02 avg=2.88\n",
            "[910 | 1704.64] loss=2.53 avg=2.88\n",
            "[911 | 1706.27] loss=3.19 avg=2.88\n",
            "[912 | 1707.90] loss=3.23 avg=2.88\n",
            "[913 | 1709.53] loss=3.18 avg=2.89\n",
            "[914 | 1711.16] loss=2.16 avg=2.88\n",
            "[915 | 1712.79] loss=2.88 avg=2.88\n",
            "[916 | 1714.42] loss=2.78 avg=2.88\n",
            "[917 | 1716.04] loss=2.97 avg=2.88\n",
            "[918 | 1717.67] loss=2.60 avg=2.88\n",
            "[919 | 1719.31] loss=2.54 avg=2.87\n",
            "[920 | 1720.93] loss=2.89 avg=2.87\n",
            "[921 | 1722.56] loss=2.86 avg=2.87\n",
            "[922 | 1724.18] loss=3.20 avg=2.88\n",
            "[923 | 1725.80] loss=2.95 avg=2.88\n",
            "[924 | 1727.43] loss=2.16 avg=2.87\n",
            "[925 | 1729.05] loss=2.64 avg=2.87\n",
            "[926 | 1730.68] loss=3.30 avg=2.87\n",
            "[927 | 1732.30] loss=3.66 avg=2.88\n",
            "[928 | 1733.93] loss=2.79 avg=2.88\n",
            "[929 | 1735.56] loss=3.07 avg=2.88\n",
            "[930 | 1737.18] loss=3.37 avg=2.89\n",
            "[931 | 1738.81] loss=2.74 avg=2.89\n",
            "[932 | 1740.43] loss=2.50 avg=2.88\n",
            "[933 | 1742.05] loss=3.13 avg=2.88\n",
            "[934 | 1743.68] loss=3.00 avg=2.89\n",
            "[935 | 1745.31] loss=2.61 avg=2.88\n",
            "[936 | 1746.93] loss=2.59 avg=2.88\n",
            "[937 | 1748.56] loss=3.20 avg=2.88\n",
            "[938 | 1750.19] loss=2.44 avg=2.88\n",
            "[939 | 1751.81] loss=2.63 avg=2.88\n",
            "[940 | 1753.44] loss=3.26 avg=2.88\n",
            "[941 | 1755.07] loss=3.13 avg=2.88\n",
            "[942 | 1756.69] loss=2.47 avg=2.88\n",
            "[943 | 1758.32] loss=2.56 avg=2.87\n",
            "[944 | 1759.95] loss=3.13 avg=2.88\n",
            "[945 | 1761.58] loss=2.64 avg=2.87\n",
            "[946 | 1763.20] loss=3.41 avg=2.88\n",
            "[947 | 1764.84] loss=2.61 avg=2.88\n",
            "[948 | 1766.47] loss=2.83 avg=2.88\n",
            "[949 | 1768.09] loss=3.02 avg=2.88\n",
            "[950 | 1769.72] loss=2.63 avg=2.88\n",
            "[951 | 1771.35] loss=3.42 avg=2.88\n",
            "[952 | 1772.98] loss=3.05 avg=2.88\n",
            "[953 | 1774.61] loss=3.19 avg=2.89\n",
            "[954 | 1776.24] loss=2.77 avg=2.88\n",
            "[955 | 1777.86] loss=2.56 avg=2.88\n",
            "[956 | 1779.48] loss=2.79 avg=2.88\n",
            "[957 | 1781.11] loss=2.68 avg=2.88\n",
            "[958 | 1782.74] loss=2.68 avg=2.88\n",
            "[959 | 1784.37] loss=2.87 avg=2.88\n",
            "[960 | 1785.99] loss=3.02 avg=2.88\n",
            "[961 | 1787.62] loss=3.18 avg=2.88\n",
            "[962 | 1789.24] loss=2.98 avg=2.88\n",
            "[963 | 1790.87] loss=3.23 avg=2.89\n",
            "[964 | 1792.49] loss=2.54 avg=2.88\n",
            "[965 | 1794.13] loss=1.91 avg=2.87\n",
            "[966 | 1795.76] loss=2.30 avg=2.87\n",
            "[967 | 1797.38] loss=2.85 avg=2.87\n",
            "[968 | 1799.01] loss=3.42 avg=2.87\n",
            "[969 | 1800.64] loss=2.89 avg=2.87\n",
            "[970 | 1802.26] loss=2.99 avg=2.87\n",
            "[971 | 1803.90] loss=3.75 avg=2.88\n",
            "[972 | 1805.52] loss=3.28 avg=2.89\n",
            "[973 | 1807.15] loss=3.57 avg=2.89\n",
            "[974 | 1808.78] loss=2.82 avg=2.89\n",
            "[975 | 1810.41] loss=2.32 avg=2.89\n",
            "[976 | 1812.04] loss=3.38 avg=2.89\n",
            "[977 | 1813.67] loss=3.06 avg=2.89\n",
            "[978 | 1815.30] loss=3.67 avg=2.90\n",
            "[979 | 1816.93] loss=2.58 avg=2.90\n",
            "[980 | 1818.55] loss=2.70 avg=2.90\n",
            "[981 | 1820.18] loss=3.26 avg=2.90\n",
            "[982 | 1821.82] loss=2.24 avg=2.89\n",
            "[983 | 1823.44] loss=2.82 avg=2.89\n",
            "[984 | 1825.07] loss=2.88 avg=2.89\n",
            "[985 | 1826.70] loss=3.11 avg=2.89\n",
            "[986 | 1828.33] loss=2.58 avg=2.89\n",
            "[987 | 1829.95] loss=3.03 avg=2.89\n",
            "[988 | 1831.58] loss=3.03 avg=2.89\n",
            "[989 | 1833.21] loss=3.34 avg=2.90\n",
            "[990 | 1834.84] loss=3.03 avg=2.90\n",
            "[991 | 1836.47] loss=2.44 avg=2.89\n",
            "[992 | 1838.10] loss=2.78 avg=2.89\n",
            "[993 | 1839.73] loss=2.58 avg=2.89\n",
            "[994 | 1841.36] loss=3.26 avg=2.89\n",
            "[995 | 1842.99] loss=2.67 avg=2.89\n",
            "[996 | 1844.62] loss=3.14 avg=2.89\n",
            "[997 | 1846.25] loss=2.69 avg=2.89\n",
            "[998 | 1847.88] loss=2.94 avg=2.89\n",
            "[999 | 1849.51] loss=1.94 avg=2.88\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " , .\n",
            "Molly !\n",
            "tell Daddy that he can come right in , and then it 's all up ,\n",
            "and I do 'll go and tell you something , if you like ! '\n",
            "` Do n't go , oh , Mr. Blythe , for I 've not the time , no , for .\n",
            "But there ought to be time for it , for you are to live with me for a very long time , and I 'm so very fond of you , you 'll never give me the time I don't wish it , and I 'm sure it 's n't right to want to see you unhappy , my fair child .\n",
            "You do want to spend a year with me ?\n",
            "Good , let us put it out to you , if you like ! '\n",
            "` Oh , it shall always come to that , because of you , my beloved darling , and I 'll always love you till I die ! '\n",
            "with all the best wishes in her voice , and for her own sake , not in the least because of the affection that Molly had for him .\n",
            "He was not at all astonished .\n",
            "For a minute he did not know what to say .\n",
            "` Tell him that yes , of course , I do want to spend the year with him .\n",
            "I do n't care how long I may have to stay , and I 'll enjoy it , you 'll be right on your way , and if the old man can wait in the window , I 'll soon leave as well , ' he answered , for he had not the power to leave till he really did , as he felt that if he went at all his little time at the old homestead would never be gone , and he was so glad to have a chance with the little man in his garden , where all the old men and all the dogs and cats were as old and familiar as himself . '\n",
            "` I think there might come a time , for instance , when you would be willing to say so , ' said Molly , in an excited mood , which at the moment of leaving the nursery door she had so little time to relax .\n",
            "`` Would n't it ?\n",
            "Yes !\n",
            "I believe so , yes , ' said the old man in an anxious manner , as he thought of the opportunity .\n",
            "Molly answered him with a smile and drew back .\n",
            "` I 'm just in the garden , and then I 'll be right behind you , dear , and I 'll come up with you .\n",
            "Oh , you do really believe that ?\n",
            "No , never would , ' answered the old man when he saw Molly walk in .\n",
            "And Molly did not even flutter her little brown hands at seeing her father leave the room , for he was a long way off , and she would not be at such pains to see him . '\n",
            "I should feel so very sorry had I done you the wrong thing when you left the nursery I would have been so upset I 'd be sure to cry myself to sleep , ' she said to herself .\n",
            "` If I did n't think there was any harm in it , I 'd not be ashamed to let you know now , ' said the old man .\n",
            "So he went back to his room , saying that he would not do such things , so good-bye and thank you for taking such pains for so long , and seeing what such a delightful child you were to have , and if it would do you any good you wo n't leave me , Mr. Blythe , I am sure it will , ' he said as he opened his door to get away , which was not to be done for years .\n",
            "` Oh , thank you for loving me so , Mr. Blythe , I ca n't put it down to my own fault , I hope I 'll get on with everything , so I will .\n",
            "I will always love you , '' thought Molly , as she pulled at her little brown skirt to show the lovely pink puffy bits , but she had no intention to do so now , for if she saw him come she would run and warn the other children .\n",
            "` I ca n't go , Miss Blythe , for I 'm going to go with you , and if I do you need look after yourself , you 'll find it very hard .\n",
            "But do n't come to see me till I ca n't leave you , dear , and then you might get yourself ill .\n",
            "What shall I do ? '\n",
            "cried Molly , as she went to her room to find that a very unhappy old man was waiting on the veranda to see her , and the first thing she felt was the little brown hat hanging on the mantelpiece in the corner .\n",
            "She had been thinking of it , and thinking of leaving the nursery , but she was so excited , so very glad , and the little hat so very happy , and she could not help the blush on her face .\n",
            "She started up as hard as\n",
            "\n",
            "[1000 | 1882.44] loss=2.75 avg=2.88\n",
            "[1001 | 1884.06] loss=2.62 avg=2.88\n",
            "[1002 | 1885.67] loss=2.85 avg=2.88\n",
            "[1003 | 1887.29] loss=3.21 avg=2.88\n",
            "[1004 | 1888.92] loss=2.88 avg=2.88\n",
            "[1005 | 1890.53] loss=2.79 avg=2.88\n",
            "[1006 | 1892.15] loss=3.04 avg=2.88\n",
            "[1007 | 1893.77] loss=3.07 avg=2.89\n",
            "[1008 | 1895.39] loss=2.27 avg=2.88\n",
            "[1009 | 1897.01] loss=3.14 avg=2.88\n",
            "[1010 | 1898.64] loss=3.25 avg=2.89\n",
            "[1011 | 1900.26] loss=2.45 avg=2.88\n",
            "[1012 | 1901.86] loss=2.99 avg=2.88\n",
            "[1013 | 1903.49] loss=2.44 avg=2.88\n",
            "[1014 | 1905.11] loss=3.11 avg=2.88\n",
            "[1015 | 1906.73] loss=3.20 avg=2.88\n",
            "[1016 | 1908.35] loss=2.62 avg=2.88\n",
            "[1017 | 1909.97] loss=3.21 avg=2.88\n",
            "[1018 | 1911.59] loss=3.49 avg=2.89\n",
            "[1019 | 1913.22] loss=2.88 avg=2.89\n",
            "[1020 | 1914.84] loss=3.50 avg=2.90\n",
            "[1021 | 1916.47] loss=2.46 avg=2.89\n",
            "[1022 | 1918.08] loss=3.00 avg=2.89\n",
            "[1023 | 1919.70] loss=3.15 avg=2.89\n",
            "[1024 | 1921.31] loss=3.27 avg=2.90\n",
            "[1025 | 1922.94] loss=2.92 avg=2.90\n",
            "[1026 | 1924.56] loss=2.83 avg=2.90\n",
            "[1027 | 1926.19] loss=2.92 avg=2.90\n",
            "[1028 | 1927.82] loss=2.88 avg=2.90\n",
            "[1029 | 1929.44] loss=3.45 avg=2.90\n",
            "[1030 | 1931.06] loss=2.89 avg=2.90\n",
            "[1031 | 1932.68] loss=2.97 avg=2.90\n",
            "[1032 | 1934.30] loss=2.53 avg=2.90\n",
            "[1033 | 1935.92] loss=3.48 avg=2.91\n",
            "[1034 | 1937.55] loss=2.55 avg=2.90\n",
            "[1035 | 1939.17] loss=2.81 avg=2.90\n",
            "[1036 | 1940.80] loss=3.21 avg=2.90\n",
            "[1037 | 1942.42] loss=3.10 avg=2.91\n",
            "[1038 | 1944.04] loss=2.96 avg=2.91\n",
            "[1039 | 1945.67] loss=2.54 avg=2.90\n",
            "[1040 | 1947.29] loss=3.48 avg=2.91\n",
            "[1041 | 1948.92] loss=2.76 avg=2.91\n",
            "[1042 | 1950.54] loss=2.78 avg=2.91\n",
            "[1043 | 1952.16] loss=2.86 avg=2.91\n",
            "[1044 | 1953.78] loss=3.48 avg=2.91\n",
            "[1045 | 1955.41] loss=2.26 avg=2.91\n",
            "[1046 | 1957.03] loss=3.52 avg=2.91\n",
            "[1047 | 1958.65] loss=3.05 avg=2.91\n",
            "[1048 | 1960.27] loss=2.53 avg=2.91\n",
            "[1049 | 1961.90] loss=2.82 avg=2.91\n",
            "[1050 | 1963.52] loss=3.72 avg=2.92\n",
            "[1051 | 1965.15] loss=2.88 avg=2.92\n",
            "[1052 | 1966.77] loss=2.39 avg=2.91\n",
            "[1053 | 1968.40] loss=3.08 avg=2.91\n",
            "[1054 | 1970.03] loss=3.31 avg=2.92\n",
            "[1055 | 1971.65] loss=3.24 avg=2.92\n",
            "[1056 | 1973.28] loss=3.05 avg=2.92\n",
            "[1057 | 1974.90] loss=3.27 avg=2.92\n",
            "[1058 | 1976.53] loss=2.74 avg=2.92\n",
            "[1059 | 1978.15] loss=2.83 avg=2.92\n",
            "[1060 | 1979.78] loss=2.47 avg=2.92\n",
            "[1061 | 1981.40] loss=2.42 avg=2.91\n",
            "[1062 | 1983.03] loss=3.29 avg=2.92\n",
            "[1063 | 1984.66] loss=3.12 avg=2.92\n",
            "[1064 | 1986.28] loss=3.21 avg=2.92\n",
            "[1065 | 1987.90] loss=2.42 avg=2.92\n",
            "[1066 | 1989.52] loss=3.23 avg=2.92\n",
            "[1067 | 1991.15] loss=2.39 avg=2.91\n",
            "[1068 | 1992.77] loss=2.76 avg=2.91\n",
            "[1069 | 1994.40] loss=2.65 avg=2.91\n",
            "[1070 | 1996.02] loss=2.69 avg=2.91\n",
            "[1071 | 1997.65] loss=3.81 avg=2.92\n",
            "[1072 | 1999.27] loss=3.07 avg=2.92\n",
            "[1073 | 2000.89] loss=3.25 avg=2.92\n",
            "[1074 | 2002.52] loss=3.12 avg=2.92\n",
            "[1075 | 2004.15] loss=2.91 avg=2.92\n",
            "[1076 | 2005.78] loss=2.37 avg=2.92\n",
            "[1077 | 2007.40] loss=3.06 avg=2.92\n",
            "[1078 | 2009.03] loss=2.78 avg=2.92\n",
            "[1079 | 2010.65] loss=2.82 avg=2.92\n",
            "[1080 | 2012.28] loss=2.85 avg=2.92\n",
            "[1081 | 2013.91] loss=2.80 avg=2.91\n",
            "[1082 | 2015.54] loss=2.14 avg=2.91\n",
            "[1083 | 2017.16] loss=2.28 avg=2.90\n",
            "[1084 | 2018.79] loss=2.48 avg=2.90\n",
            "[1085 | 2020.42] loss=3.13 avg=2.90\n",
            "[1086 | 2022.05] loss=2.71 avg=2.90\n",
            "[1087 | 2023.67] loss=3.21 avg=2.90\n",
            "[1088 | 2025.29] loss=3.19 avg=2.90\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-1089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6lFJpX4rnyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the checkpoints in the google drive\n",
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv8_wNrO1Lyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH5TEsAS1eJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1YCttLR1g8B",
        "colab_type": "text"
      },
      "source": [
        "Generating Conditional Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ROaFOk1n0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "4370f985-2290-4994-d423-fecd66206438"
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\t\t\t    LICENSE\n",
            "CONTRIBUTORS.md\t\t\t\t\t    models\n",
            "cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb  README.md\n",
            "DEVELOPERS.md\t\t\t\t\t    requirements.txt\n",
            "Dockerfile.cpu\t\t\t\t\t    samples\n",
            "Dockerfile.gpu\t\t\t\t\t    src\n",
            "download_model.py\t\t\t\t    train-horovod.py\n",
            "encode.py\t\t\t\t\t    train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GnWyaTE1pKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a431d01-81ed-4a62-b279-95b475f2699d"
      },
      "source": [
        "import os\n",
        "%cd src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIOpdInw151-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "03e7eac4-e71f-4077-f5e4-6bc0d305ec3d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accumulate.py\t      generate_unconditional_samples.py   model.py\n",
            "conditional_model.py  interactive_conditional_samples.py  __pycache__\n",
            "corpus\t\t      load_dataset.py\t\t\t  sample.py\n",
            "encoder.py\t      memory_saving_gradients.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0otTxM-a17Kg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "d4d1dea9-c63a-4845-e771-a3b44d48d43c"
      },
      "source": [
        "from conditional model import conditional_model\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-1822c5c65485>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from conditional model import conditional_model\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebhMAE4h2K4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5d66f75-19f5-43aa-b54c-8b2f8aeed662"
      },
      "source": [
        "from conditional_model import conditional_model\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4gOTtd72T46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "8aed7ae8-376b-4ab3-f504-f5ca11f11b8f"
      },
      "source": [
        "conditional_model(seed=1,sentences=['How are you today?'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/conditional_model.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/conditional_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/conditional_model.py:62: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/conditional_model.py:70: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from models/345M/model-1089\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'How are you today?': \"How are you today?\\nWhy are you up at three o'clock in the morning ?\\nYou 're not coming home, are you ?\\nI 'm afraid of you , Bob ! ''\\nHe had just put a foot on the couch and was speaking up on one side when he remembered `` the red face of Mrs. Lynde .\\nWhy can she smell so of tobacco ? ''\\n`` Not exactly , '' began Bob impatiently , `` but there 'll be smoke at least , as well as the smell of tea in Mrs. Lynde 's cup ; and when Mrs. Lynde 's smoke comes into the lounge rooms , the smell is much the same as smoke in the lounge rooms of every other family .\\nNo one ever wants to know which the mother 's is .\\nThey 're too frightened of the smoke .\\nSo I suppose the woman who has that nasty '' -Bob looked at him intently -`` smoke smell is the mother 's ! ''\\nblessed and proud smile appeared on Linda 's face again ; and Bob began to be an amusingly bitter man , when he could be an unhappy man indeed -`` I 'll be in very bad shape in a few days when my nose is all over so easily . ''\\n`` Well , then , perhaps it would surprise you if you did come down here -- and see to the cleaning up .\\nThat is something that needs doing .\\nAnd you will , Bob ! ''\\n`` I am a very busy father and want to start right off , '' said Bob sadly .\\n`` But when I go along , it is no longer about what my boy is doing .\\nThere must be something else to worry about .\\nIt 's awfully hard to run away . ''\\nLinda suddenly became angry .\\n`` You had better run away , or I 'll take care of you . ''\\n`` I do n't want to run away , Linda .\\nThat will do , but I do not want my son to take care of me .\\nYou are the one who wants to run away .\\nWhat do you think ?\\nHow can Mrs. Lynde leave me alone ? ''\\n`` She ca n't !\\nShe only wishes you would run away !\\nI tell you , Linda , it will n't do to run away .\\nI ca n't , or else I do n't run away .\\nAnd you can see it all .\\nYou are too busy , and all you 've got on your shoulders\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NaIYDvG2dkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}